# app_main.py
# CORE PYTHON IMPORTS FIRST
import os
import trafilatura
import json
import threading
import time
import queue
import re
import tempfile
import base64
import webbrowser
from collections import deque
from datetime import datetime, timedelta
from urllib.parse import urljoin
from dataclasses import dataclass, field
from typing import List, Optional

# TKINTER IMPORTS
import tkinter as tk
from tkinter import ttk, messagebox
from tkinter.scrolledtext import ScrolledText

# SCIENTIFIC/NUMERICAL IMPORTS
import numpy as np

# AUDIO/IMAGE IMPORTS
import sounddevice as sd
import soundfile as sf
from PIL import Image, ImageTk
# Add these for ImageWindow
try:
    from tkinterdnd2 import DND_FILES, TkinterDnD  # pip install tkinterdnd2
except Exception:
    DND_FILES = None
    TkinterDnD = None

try:
    import cv2  # pip install opencv-python
except Exception:
    cv2 = None

# NETWORK IMPORTS
import requests
import httpx
# Import the math speech converter
from Speak_Maths import MathSpeechConverter

# Create a global instance
math_speech_converter = MathSpeechConverter()

from web_search_window import WebSearchWindow


# EXTERNAL MODULE IMPORTS
try:
    from audio_io import list_input_devices, VADListener
    from asr_whisper import ASR
    from qwen_llmSearch2 import QwenLLM
    from pydub import AudioSegment
    from Speak_Maths import MathSpeechConverter
    from router import CommandRouter
    from Avatars import CircleAvatarWindow, RectAvatarWindow, RectAvatarWindow2, RadialPulseAvatar, FaceRadialAvatar, \
        StringGridAvatar, TextureMappedSphere
    from latex_window import LatexWindow
    from status_light_window import StatusLightWindow
except ImportError as e:
    print(f"Warning: Could not import some modules: {e}")

# ECHO ENGINE IMPORT (must come after all the imports it needs)
from echo_engine import EchoEngine, EchoWindow
# === HELPER FUNCTIONS ===
def load_cfg():
    import os, json
    env_path = os.environ.get("APP_CONFIG")
    if env_path and os.path.exists(env_path):
        path = env_path
    else:
        base = os.path.dirname(os.path.abspath(__file__))
        c1 = os.path.join(base, "config.json")
        c2 = os.path.join(base, "config.example.json")
        c3 = "config.json" if os.path.exists("config.json") else None
        c4 = "config.example.json" if os.path.exists("config.example.json") else None
        path = next((p for p in (c1, c2, c3, c4) if p and os.path.exists(p)), None)

    if not path:
        raise FileNotFoundError("No config.json or config.example.json found")

    with open(path, "r", encoding="utf-8") as f:
        cfg = json.load(f)

    print(f"[cfg] loaded: {os.path.abspath(path)}")
    sp = cfg.get("system_prompt", cfg.get("qwen_system_prompt", ""))
    print(f"[cfg] system_prompt present: {bool(sp)} (len={len(sp) if isinstance(sp, str) else 'n/a'})")
    return cfg

def clean_model_output(text: str) -> str:
    """Clean model-specific formatting artifacts from AI responses."""
    if not text:
        return ""

    cleaned = text
    print(f"üîß [CLEANER] Input: {repr(text[:100])}")

    # Remove ALL variants of DeepSeek tokens
    end_patterns = [
        '<|im_end|>',
        '<|im_end>|<think>',
        '<|end|>',
        '<|endoftext|>'
    ]

    for pattern in end_patterns:
        if pattern in cleaned:
            parts = cleaned.split(pattern)
            cleaned = parts[0].strip()
            print(f"üîß [CLEANER] Split by pattern: {pattern}")
            break

    # Remove any remaining individual tokens
    tokens_to_remove = [
        '<|im_start|>', '<|im_end|>', '<|end|>', '<|endoftext|>',
        '<|im_end>|<think>', '<|think|>', '<|system|>', '<|user|>', '<|assistant|>'
    ]

    for token in tokens_to_remove:
        cleaned = cleaned.replace(token, '')

    # Aggressive regex for any <|...|> or <|...> patterns
    cleaned = re.sub(r'<\|[^>]*(?:\|>|>)', '', cleaned)

    # Remove LaTeX document wrappers
    if '\\documentclass' in cleaned:
        if '\\begin{document}' in cleaned:
            parts = cleaned.split('\\begin{document}')
            if len(parts) > 1:
                cleaned = parts[1].strip()
        if '\\end{document}' in cleaned:
            cleaned = cleaned.split('\\end{document}')[0].strip()

    # Final cleanup
    cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
    cleaned = cleaned.strip()

    print(f"üîß [CLEANER] Output: {repr(cleaned[:100])}")
    print(f"üîß [CLEANER] Success: {not any(token in cleaned for token in tokens_to_remove)}")

    return cleaned

def purge_temp_images(folder="out"):
    """Remove temporary/snapshot images we create. Keeps non-image files."""
    try:
        if not os.path.isdir(folder):
            return
        for name in os.listdir(folder):
            low = name.lower()
            # keep audio; delete our snapshots and temp frames
            if low.startswith("snapshot_") and low.endswith(".png"):
                try:
                    os.remove(os.path.join(folder, name))
                except Exception:
                    pass
            if low in ("live_frame.png", "tmp_frame.png"):
                try:
                    os.remove(os.path.join(folder, name))
                except Exception:
                    pass
    except Exception as e:
        print(f"[startup] purge_temp_images: {e}")

        ###################


# === BRAVE SEARCH METHODS ===

# === SEARCH METHODS ===

    def brave_search(self, query: str, count: int = 6):
        brave_key = os.getenv("BRAVE_KEY")
        if not brave_key:
            raise RuntimeError("No BRAVE_KEY found in environment")
        # === Logs we are searching the Internet ===
        self.logln(f"[SEARCH] üöÄ Calling Brave API: '{query}'")

        endpoint = "https://api.search.brave.com/res/v1/web/search"
        headers = {"X-Subscription-Token": brave_key, "User-Agent": "LocalAI-ResearchBot/1.0"}
        params = {"q": query, "count": count}

        with httpx.Client(timeout=25.0, headers=headers) as client:
            r = client.get(endpoint, params=params)
            r.raise_for_status()
            data = r.json()

        out = []
        for w in (data.get("web", {}) or {}).get("results", []):
            out.append(
                Item(title=w.get("title", "No title"), url=w.get("url", ""), snippet=w.get("description", "")))
            # === check what its searching  ===
            self.logln(f"[BRAVE API] ‚úÖ Found {len(out)} results for '{query}'")

        return out

    def polite_fetch(self, url: str):
        headers = {"User-Agent": "LocalAI-ResearchBot/1.0"}
        try:
            with httpx.Client(timeout=25.0, headers=headers, follow_redirects=True) as client:
                r = client.get(url)
                r.raise_for_status()
                return r.text
        except Exception:
            return None

    def extract_readable(self, html: str, url: str = None):
        text = trafilatura.extract(html, url=url, include_links=False, include_formatting=False)
        return text or ""

    def guess_pubdate(self, html: str):
        try:
            soup = BeautifulSoup(html, "lxml")
        except Exception:
            return None

        metas = [
            ("property", "article:published_time"), ("property", "og:published_time"),
            ("property", "og:updated_time"), ("name", "pubdate"), ("name", "publication_date"),
            ("name", "date"), ("name", "dc.date"), ("name", "dc.date.issued"),
            ("name", "sailthru.date"), ("itemprop", "datePublished"), ("itemprop", "dateModified"),
        ]

        for key, val in metas:
            tag = soup.find("meta", attrs={key: val})
            if tag and tag.get("content"):
                return tag["content"]

        t = soup.find("time")
        if t and (t.get("datetime") or (t.text and t.text.strip())):
            return t.get("datetime") or t.text.strip()
        return None

    def summarise_for_ai_search(self, text: str, url: str, pubdate: str):
        """Enhanced summarization that preserves practical information"""
        text = text[:18000]

        # Enhanced date context
        if pubdate:
            date_context = f"PUBLICATION DATE: {pubdate}\n"
        else:
            import re
            date_matches = re.findall(
                r'\b(?:20\d{2}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* 20\d{2})\b',
                text[:3000])
            date_context = f"MENTIONED DATES: {', '.join(date_matches[:3])}\n" if date_matches else ""

        # DETECT QUERY TYPE AND ADAPT SUMMARIZATION
        query_lower = getattr(self, '_last_search_query', '').lower()

        # Flight/travel related queries
        if any(keyword in query_lower for keyword in ['flight', 'fly', 'airline', 'airport', 'travel to']):
            summary_prompt = (
                "Extract COMPLETE flight information with these details:\n\n"
                "## FLIGHT INFORMATION\n"
                "- Airline names and flight numbers\n"
                "- Departure and arrival airports (with codes if available)\n"
                "- Departure and arrival times/dates\n"
                "- Flight duration\n"
                "- Prices and fare classes\n"
                "- Stopovers/layovers\n"
                "- Booking links or airline websites\n\n"
                "## TRAVEL DETAILS\n"
                "- Airport locations and terminals\n"
                "- Booking requirements\n"
                "- Baggage information\n"
                "- Recent deals or promotions\n\n"
                "Include ALL specific numbers, times, prices, and codes. Be very detailed about schedules and availability.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        # Business/location queries
        elif any(keyword in query_lower for keyword in
                 ['address', 'location', 'where is', 'hours', 'contact', 'phone', 'email']):
            summary_prompt = (
                "EXTRACT ONLY INFORMATION EXPLICITLY STATED IN THE TEXT. NEVER CREATE PLACEHOLDERS OR INVENT INFORMATION.\n\n"
                "CRITICAL RULES:\n"
                "1. ONLY include information that appears VERBATIM in the source text\n"
                "2. NEVER use brackets [ ], parentheses ( ), or placeholder text\n"
                "3. If a website is mentioned, copy the EXACT URL\n"
                "4. If information is missing, OMIT that line entirely\n"
                "5. Do NOT create template responses\n\n"
                "EXTRACTED INFORMATION (ONLY IF FOUND):\n"
                "- Business Name: [copy exact name if found]\n"
                "- Address: [copy exact address if found]\n"
                "- Phone: [copy exact phone number if found]\n"
                "- Email: [copy exact email if found]\n"
                "- Website: [copy exact URL if found]\n"
                "- Hours: [copy exact hours if found]\n\n"
                "EXAMPLES - WRONG:\n"
                "‚ùå Address: [Address may vary]\n"
                "‚ùå Phone: [Phone number may vary]  \n"
                "‚ùå Website: [Website Link]\n"
                "‚ùå Website: [Website URL if available]\n\n"
                "EXAMPLES - CORRECT:\n"
                "‚úÖ Address: 456 Northshore Road, Unit 2, Glenfield 0678\n"
                "‚úÖ Phone: +64 9 483 5555\n"
                "‚úÖ Website: https://www.serenityspa.co.nz\n"
                "‚úÖ Website: www.serenityspa.com\n"
                "‚úÖ (omit Website line if no URL found)\n\n"
                "If the text contains '456 Glenfield Road, Unit 2, Glenfield 0678' and '+64 9 483 5555' but NO website, output:\n"
                "Address: 456 Glenfield Road, Unit 2, Glenfield 0678\n"
                "Phone: +64 9 483 5555\n\n"
                "DO NOT INVENT WEBSITE INFORMATION. If no website is found, omit the Website line completely.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )


        # Product/service queries
        elif any(keyword in query_lower for keyword in ['price', 'cost', 'buy', 'purchase', 'deal', 'sale']):
            summary_prompt = (
                "Extract COMPLETE product/service information:\n\n"
                "## PRICING & AVAILABILITY\n"
                "- Exact prices and currency\n"
                "- Model numbers/specifications\n"
                "- Availability status\n"
                "- Seller/retailer information\n"
                "- Shipping costs and delivery times\n"
                "- Return policies\n\n"
                "## PRODUCT DETAILS\n"
                "- Features and specifications\n"
                "- Dimensions/sizes\n"
                "- Colors/options available\n"
                "- Warranty information\n\n"
                "Include ALL pricing, specifications, and purchase details. Be very specific about numbers and options.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        # Weather queries
        elif any(keyword in query_lower for keyword in
                 ['weather', 'forecast', 'temperature', 'rain', 'snow', 'humidity']):
            summary_prompt = (
                "Extract COMPLETE weather forecast information:\n\n"
                "## CURRENT CONDITIONS\n"
                "- Temperature and feels-like temperature\n"
                "- Weather description (sunny, rainy, etc.)\n"
                "- Humidity, wind speed and direction\n"
                "- Precipitation chances\n"
                "- Air quality and UV index\n\n"
                "## FORECAST\n"
                "- Hourly and daily forecasts\n"
                "- High/low temperatures\n"
                "- Severe weather alerts\n"
                "- Sunrise/sunset times\n\n"
                "## LOCATION DETAILS\n"
                "- Specific city/region\n"
                "- Geographic details if available\n"
                "- Timezone information\n\n"
                "Include ALL numerical weather data, times, and location specifics.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        else:
            # General comprehensive summary (for news, general info, etc.)
            summary_prompt = (
                "Create a COMPREHENSIVE summary that PRESERVES practical information:\n\n"
                "## ESSENTIAL DETAILS\n"
                "- Full names of businesses, people, organizations\n"
                "- Complete addresses, phone numbers, contact information\n"
                "- Prices, costs, financial figures\n"
                "- Dates, times, schedules\n"
                "- Locations, coordinates, directions\n"
                "- Website URLs, email addresses\n\n"
                "## KEY INFORMATION\n"
                "- Main facts and findings\n"
                "- Important numbers and statistics\n"
                "- Recent developments\n"
                "- Contact methods\n\n"
                "## ADDITIONAL CONTEXT\n"
                "- Background information\n"
                "- Related services or options\n"
                "- User reviews or ratings if available\n\n"
                "CRITICAL: NEVER omit addresses, phone numbers, prices, or contact information. Include them verbatim.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        try:
            payload = {
                "model": "qwen2.5:7b-instruct",
                "prompt": summary_prompt,
                "stream": False,
                "temperature": 0.1,  # Lower temperature for more factual accuracy
                "max_tokens": 1200  # More tokens for detailed information
            }

            with httpx.Client(timeout=75.0) as client:
                r = client.post("http://localhost:11434/api/generate", json=payload)
                r.raise_for_status()
                response = r.json().get("response", "").strip()

                # Enhanced fallback for better information extraction
                if len(response) < 100 or "no information" in response.lower():
                    return self._extract_practical_information(text[:12000], query_lower)

                return response

        except Exception as e:
            return self._extract_practical_information(text[:10000], query_lower)

    def _extract_practical_information(self, text: str, query_type: str) -> str:
        """Enhanced fallback extraction focusing on practical information"""
        import re

        sections = []

        # Enhanced address extraction
        address_patterns = [
            # Standard street addresses
            r'\b\d+\s+[A-Za-z0-9\s,]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct|Place|Pl|Way|Highway|Hwy)\.?\s*(?:#\s*\d+)?\s*,\s*[A-Za-z\s]+,\s*[A-Z]{2}\s*\d{5}(?:-\d{4})?\b',
            # Basic address format
            r'\b\d+\s+[\w\s]+\s+(?:St|Street|Ave|Avenue|Rd|Road|Blvd|Boulevard|Dr|Drive|Ln|Lane|Ct|Court),\s*[\w\s]+,\s*[A-Z]{2}\b',
            # PO Boxes
            r'\b(?:P\.?O\.?\s*Box|PO Box|P O Box)\s+\d+[^.!?]*',
        ]

        addresses = []
        for pattern in address_patterns:
            found = re.findall(pattern, text, re.IGNORECASE)
            addresses.extend(found)

        # Filter out obviously fake or placeholder addresses
        real_addresses = []
        for addr in addresses:
            addr_lower = addr.lower()
            # Skip placeholder text
            if any(placeholder in addr_lower for placeholder in
                   ['address may vary', 'varies', 'please contact', 'call for', 'not available']):
                continue
            # Skip if it's just a city/state without street
            if re.match(r'^[A-Za-z\s]+,?\s*[A-Z]{2}\s*\d{5}', addr) and not re.search(r'\d+', addr):
                continue
            real_addresses.append(addr.strip())

        if real_addresses:
            sections.append("## ADDRESSES FOUND")
            sections.extend([f"- {addr}" for addr in set(real_addresses)[:3]])
        # Extract website URLs (more comprehensive)
        url_patterns = [
            r'https?://[^\s<>"{}|\\^`\[\]]+',
            r'www\.[^\s<>"{}|\\^`\[\]]+\.[a-z]{2,}',
            r'[a-z0-9.-]+\.[a-z]{2,}/[^\s<>"{}|\\^`\[\]]*',
        ]

        urls = []
        for pattern in url_patterns:
            urls.extend(re.findall(pattern, text, re.IGNORECASE))

        # Filter and clean URLs
        clean_urls = []
        for url in urls:
            # Remove trailing punctuation
            url = re.sub(r'[.,;:!?)]+$', '', url)
            # Skip common false positives
            if any(bad in url.lower() for bad in ['example.com', 'website.com', 'yourwebsite', 'domain.com']):
                continue
            # Ensure it looks like a real URL
            if '.' in url and len(url) > 8:
                # Add http:// if missing for www URLs
                if url.startswith('www.') and not url.startswith('http'):
                    url = 'https://' + url
                clean_urls.append(url)

        if clean_urls:
            sections.append("\n## WEBSITES")
            sections.extend([f"- {url}" for url in set(clean_urls)[:3]])

        # Extract phone numbers
        phone_pattern = r'(\+?\d{1,2}?[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})'
        phones = re.findall(phone_pattern, text)
        if phones:
            sections.append(f"\n## PHONE NUMBERS: {', '.join(set(phones)[:3])}")

        # Extract email addresses
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
        if emails:
            sections.append(f"\n## EMAIL ADDRESSES: {', '.join(set(emails)[:3])}")

        # Extract prices and costs
        prices = re.findall(r'\$?\d+(?:,\d+)*(?:\.\d+)?\s*(?:dollars?|USD|‚Ç¨|¬£|¬•)?', text)
        if prices:
            sections.append(f"\n## PRICES MENTIONED: {', '.join(set(prices)[:8])}")

        # Flight-specific extraction
        if 'flight' in query_type:
            flight_info = re.findall(r'[A-Z]{2}\d+\s+.*?(?:\d{1,2}:\d{2}|AM|PM)', text)
            if flight_info:
                sections.append("\n## FLIGHT DETAILS")
                sections.extend([f"- {info}" for info in flight_info[:5]])

        # Business hours
        hours = re.findall(
            r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)[a-z]*.*?\d{1,2}:\d{2}\s*(?:AM|PM)?.*?\d{1,2}:\d{2}\s*(?:AM|PM)?', text,
            re.IGNORECASE)
        if hours:
            sections.append("\n## BUSINESS HOURS")
            sections.extend([f"- {hour}" for hour in hours[:3]])

        # Weather data extraction
        if 'weather' in query_type:
            temps = re.findall(r'\b\d{1,3}¬∞?F?\b', text)
            if temps:
                sections.append(f"\n## TEMPERATURES: {', '.join(set(temps)[:6])}")

        # If we found practical information, return it
        if sections:
            return "\n".join(sections)
        else:
            # Return meaningful content lines as fallback
            lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 30]
            return "## KEY INFORMATION EXTRACTED\n" + "\n".join([f"- {line}" for line in lines[:10]])

    def _extract_detailed_news(self, text: str) -> str:
        """Enhanced fallback extraction with more structure"""
        import re

        # Extract key information with more context
        sections = []

        # Headlines and key sentences
        sentences = re.split(r'[.!?]+', text)
        key_sentences = []

        important_indicators = [
            'announced', 'reported', 'confirmed', 'revealed', 'disclosed',
            'investigation', 'charged', 'arrested', 'settlement', 'agreement',
            'election', 'resigned', 'appointed', 'launched', 'released',
            'fire', 'accident', 'killed', 'injured', 'missing', 'found',
            'storm', 'flood', 'earthquake', 'weather', 'forecast', 'temperature',
            'budget', 'funding', 'cost', 'price', 'investment'
        ]

        for sentence in sentences:
            sentence = sentence.strip()
            if (len(sentence) > 25 and
                    any(indicator in sentence.lower() for indicator in important_indicators)):
                key_sentences.append(sentence)
                if len(key_sentences) >= 12:
                    break

        if key_sentences:
            sections.append("## KEY DEVELOPMENTS")
            sections.extend([f"- {s}" for s in key_sentences[:10]])

        # Extract numbers and statistics
        numbers = re.findall(r'\b(\$?[¬£‚Ç¨]?\d+(?:,\d+)*(?:\.\d+)?[%‚Ç¨¬£$]?(?:\s*(?:million|billion|thousand))?)\b',
                             text[:5000])
        if numbers:
            sections.append(f"\n## KEY NUMBERS: {', '.join(set(numbers[:8]))}")

        # Extract locations
        locations = re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b', text[:3000])
        unique_locs = list(
            set([loc for loc in locations if len(loc) > 3 and loc not in ['The', 'This', 'That', 'There', 'Here']]))
        if unique_locs:
            sections.append(f"\n## MENTIONED LOCATIONS: {', '.join(unique_locs[:6])}")

        if sections:
            return "\n".join(sections)
        else:
            # Last resort: return structured excerpt
            lines = text.split('\n')
            meaningful_lines = [line.strip() for line in lines if len(line.strip()) > 40][:8]
            return "## CONTENT OVERVIEW\n" + "\n".join([f"- {line}" for line in meaningful_lines])

    def summarise_with_qwen(self, text: str, url: str, pubdate: str):
        text = text[:20000]  # Limit text length
        pd_line = f"(Publish/Update date: {pubdate})\n" if pubdate else ""

        # FIRST PASS: Extract mathematical content specifically
        math_prompt = (
            "Extract ALL mathematical equations, formulas, and technical content from the following text. "
            "Preserve them exactly in their original LaTeX format ($$...$$, \\[...\\], $...$, etc.).\n"
            "Include:\n"
            "- All equations and formulas\n"
            "- Mathematical expressions\n"
            "- Chemical formulas\n"
            "- Code snippets\n"
            "- Important technical definitions\n"
            "Output the mathematical/technical content exactly as found, without summarization.\n"
            f"{pd_line}"
            f"Source: {url}\n\nCONTENT:\n{text[:10000]}"  # Use first 10k chars for math extraction
        )

        # SECOND PASS: Create a summary that REFERENCES the preserved math
        summary_prompt = (
            "Create a comprehensive summary (10-15 bullet points) that includes:\n"
            "- Key findings and conclusions\n"
            "- Important data points and results\n"
            "- References to mathematical content (say 'see equation X' or 'the formula shows')\n"
            "- Main arguments and evidence\n"
            "- Do NOT remove technical details - include them in context\n"
            "- Preserve specific numbers, measurements, and quantitative results\n"
            "Be detailed enough to be useful for technical analysis.\n"
            f"{pd_line}"
            f"Source: {url}\n\nCONTENT:\n{text}"
        )

        try:
            # Get mathematical content
            math_content = self.qwen.generate(math_prompt)

            # Get comprehensive summary
            summary = self.qwen.generate(summary_prompt)

            # Combine both with clear separation
            combined_result = f"MATHEMATICAL CONTENT:\n{math_content}\n\nSUMMARY:\n{summary}"

            return combined_result

        except Exception:
            # Fallback: Use a more math-friendly single prompt
            fallback_prompt = (
                "Create a DETAILED technical summary (12-18 bullet points) that PRESERVES all mathematical content.\n"
                "CRITICAL: Keep ALL equations, formulas, and LaTeX expressions exactly as they appear.\n"
                "Include:\n"
                "- Complete equations in $$...$$, \\[...\\], $...$ format\n"
                "- Mathematical proofs and derivations\n"
                "- Chemical formulas and reactions\n"
                "- Code snippets and algorithms\n"
                "- Quantitative results with exact numbers\n"
                "- Do NOT simplify or remove technical details\n"
                "- Focus on preserving the mathematical richness of the content\n"
                f"{pd_line}"
                f"Source: {url}\n\nCONTENT:\n{text}"
            )
            try:
                payload = {"model": "qwen2.5:7b-instruct", "prompt": fallback_prompt, "stream": False}
                with httpx.Client(timeout=90.0) as client:
                    r = client.post("http://localhost:11434/api/generate", json=payload)
                    r.raise_for_status()
                    return r.json().get("response", "").strip()
            except Exception as e:
                return f"Summarization failed: {e}"

    def extract_images(self, html: str, base_url: str, limit: int = 3):
        urls = []
        try:
            soup = BeautifulSoup(html, "lxml")
        except Exception:
            return urls

        for img in soup.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:") or re.search(r"\.svg($|\?)", src, re.I):
                continue

            alt = (img.get("alt") or "").lower()
            src_l = src.lower()
            if any(k in src_l for k in ["sprite", "icon", "logo", "ads", "advert", "pixel"]):
                continue
            if any(k in alt for k in ["icon", "logo"]):
                continue

            full = urljoin(base_url, src)
            if full not in urls:
                urls.append(full)
            if len(urls) >= limit:
                break
        return urls

    def synthesize_search_results(self, text: str):
        """Speak search results using DEDICATED search window"""

        # === STOP PROGRESS INDICATOR IMMEDIATELY ===
        self.stop_search_progress_indicator()

        def _tts_worker():
            if not text or not text.strip():
                return

            try:
                # Use math speaking for search results too
                speak_math = getattr(self, 'speak_math_var', tk.BooleanVar(value=True)).get()
                clean_tts_text = clean_for_tts(text, speak_math=speak_math)

                # === CRITICAL: Use DEDICATED search window ===
                self.preview_search_results(text)

                # Continue with TTS...
                output_path = "out/search_results.wav"

                if self.synthesize_to_wav(clean_tts_text, output_path, role="text"):
                    with self._play_lock:
                        self._play_token += 1
                        my_token = self._play_token
                        self.interrupt_flag = False
                        self.speaking_flag = True

                    self.set_light("speaking")

                    play_path = output_path
                    if bool(self.echo_enabled_var.get()):
                        try:
                            play_path, _ = self.echo_engine.process_file(output_path, "out/search_results_echo.wav")
                            self.logln("[echo] processed search results -> out/search_results_echo.wav")
                        except Exception as e:
                            self.logln(f"[echo] processing failed: {e} (playing dry)")

                    self.play_wav_with_interrupt(play_path, token=my_token)

            except Exception as e:
                self.logln(f"[search][TTS] Error: {e}")
            finally:
                self.speaking_flag = False
                self.interrupt_flag = False
                self.set_light("idle")

        tts_thread = threading.Thread(target=_tts_worker, daemon=True)
        tts_thread.start()

    # End syththesise_search

    def play_search_results(self, path: str, token=None):
        """Play search results audio with proper interrupt support"""
        try:
            # Use the existing playback infrastructure with token support
            with self._play_lock:
                self._play_token += 1
                my_token = self._play_token
                self.interrupt_flag = False
                self.speaking_flag = True

            self.set_light("speaking")
            self.temporary_mute_for_speech("text")  # Search uses text AI voice
            self.play_wav_with_interrupt(path, token=my_token)

        except Exception as e:
            self.logln(f"[search][playback] Error: {e}")
        finally:
            self.speaking_flag = False
            self.interrupt_flag = False
            self.set_light("idle")

    def normalize_query(self, q: str) -> str:
        """Add date context ONLY for specific time-related queries"""
        ql = q.lower()
        now = datetime.now()

        # Only add dates for explicit time references
        if "today" in ql:
            q += " " + now.strftime("%Y-%m-%d")
        elif "yesterday" in ql:
            q += " " + (now - timedelta(days=1)).strftime("%Y-%m-%d")
        elif "this week" in ql:
            q += " " + now.strftime("week %G-W%V")
        # DON'T add dates for "latest", "recent", "current" etc.

        return q


# === END SEARCH METHODS ===


def clean_for_tts(text: str, speak_math: bool = True) -> str:
    """
    Enhanced TTS cleaner that converts LaTeX math to spoken English.
    """
    if not text:
        return ""

    # Use the math speech converter to handle LaTeX math
    cleaned_text = math_speech_converter.make_speakable_text(text, speak_math=speak_math)

    # Additional light cleanup for TTS
    cleaned_text = re.sub(r"[#*_`~>\[\]\(\)-]", "", cleaned_text)
    cleaned_text = re.sub(r":[a-z_]+:", "", cleaned_text)
    cleaned_text = re.sub(r"^[QAqa]:\s*", "", cleaned_text)
    cleaned_text = re.sub(r"\s{2,}", " ", cleaned_text)

    return cleaned_text.strip()

# === ECHO ENGINE HELPER FUNCTIONS ===
def _read_wav_mono(path):
    x, sr = sf.read(path)
    if x.ndim > 1:
        x = x.mean(axis=1)
    return x.astype(np.float32), sr

def _write_wav(path, y, sr):
    peak = float(np.max(np.abs(y))) if y.size else 0.0
    if peak > 1.0:
        y = y / peak
    os.makedirs(os.path.dirname(path), exist_ok=True)
    sf.write(path, y.astype(np.float32), sr)

# === DATACLASS ===
@dataclass
class Item:
    title: str
    url: str
    snippet: str = ""
    pubdate: Optional[str] = None
    summary: Optional[str] = None
    image_urls: List[str] = field(default_factory=list)


# === MAIN APP CLASS ===
class App:

    def __init__(self, master):
        self.avatar_win = None
        self.cfg = load_cfg()

        # === Initialize logln method FIRST ===
        def logln(msg):
            def _append():
                try:
                    self.log.insert("end", msg + "\n")
                    self.log.see("end")
                except Exception:
                    pass

            try:
                self.master.after(0, _append)
            except Exception:
                pass

        self.logln = logln

        # NOW we can use logln
        self.logln(f"[cfg] qwen_model_path -> {self.cfg.get('qwen_model_path')!r}")

        # Vision system prompt
        self.vl_system_prompt = self.cfg.get(
            "vl_system_prompt",
            "You are the vision AI assistant. You CAN see the provided image(s). "
            "When describing what you see, begin with 'I am the vision AI, I can see the following: ' "
            "and then provide a clear, detailed description. "
            "Answer using only what is visible in the image plus the user question. "
            "If asked to count, return a number. If asked to identify, name the most likely class. "
            "Do not say you are text-based."
        )
        self.vl_model = (
                self.cfg.get("vl_model")
                or self.cfg.get("vl_model_path")
                or "qwen2.5-vl:7b"
        )

        # Initialize command router
        self.command_router = CommandRouter(self)


        # === ADD SLEEP VARIABLES RIGHT HERE ===



        self.master = master
        master.title("Always Listening ‚Äî Qwen (local)")
        master.geometry("1080x600")

        # === ADD MODEL SELECTION VARIABLES RIGHT HERE ===

        self.text_model_var = tk.StringVar()
        self.vision_model_var = tk.StringVar()

        # === INITIALIZE ALL TKINTER VARIABLES HERE ===
        self.tts_engine = tk.StringVar(value="sapi5")
        self.speech_rate_var = tk.IntVar(value=0)
        self.sapi_voice_var = tk.StringVar()
        self.echo_enabled_var = tk.BooleanVar(value=False)
        self.ducking_enable = tk.BooleanVar(value=True)
        self.duck_db = tk.DoubleVar(value=12.0)
        self.duck_thresh = tk.DoubleVar(value=1400.0)
        self.duck_attack = tk.IntVar(value=50)
        self.duck_release = tk.IntVar(value=250)
        self.duck_var = tk.DoubleVar(value=100.0)
        self.rms_var = tk.StringVar(value="RMS: 0")
        self.state = tk.StringVar(value="idle")
        self.device_idx = tk.StringVar()
        self.out_device_idx = tk.StringVar()
        self.duplex_mode = tk.StringVar(value="Half-duplex")
        self.latex_auto = tk.BooleanVar(value=True)
        self.latex_append_mode = tk.BooleanVar(value=False)
        self.speak_math_var = tk.BooleanVar(value=True)
        self.avatar_kind = tk.StringVar(value="Rings")

        self._last_search_query = ""
        self.search_win = None

        # External light window
        self.external_light_win = None

        # === SEARCH PROGRESS VARIABLES ===
        self._search_in_progress = False
        self._search_progress_timer = None
        self._search_progress_count = 0
        self._last_search_progress_time = 0
        # === END SEARCH PROGRESS VARIABLES ===

        # === NEW: Unified playback fencing ===
        self._play_lock = threading.Lock()
        self._play_token = 0
        # === Append Mode ====
        self.latex_append_mode = tk.BooleanVar(value=False)

        # --- UI State ---
        self.state = tk.StringVar(value="idle")
        self.running = False
        self.device_idx = tk.StringVar()
        self.out_device_idx = tk.StringVar()
        self.duplex_mode = tk.StringVar(value="Half-duplex")

        # Echo engine
        self.echo_engine = EchoEngine()
        self.echo_enabled_var = tk.BooleanVar(value=False)
        self._echo_win = None

        # Ducking controls
        self.ducking_enable = tk.BooleanVar(value=True)
        self.duck_db = tk.DoubleVar(value=12.0)
        self.duck_thresh = tk.DoubleVar(value=1400.0)
        self.duck_attack = tk.IntVar(value=50)
        self.duck_release = tk.IntVar(value=250)
        self._duck_gain = 1.0
        self._duck_active = False

        self._current_speaker = None  # Track which AI is currently speaking
        self._speaker_lock = threading.Lock()

        self._duck_log = bool(self.cfg.get("duck_log", False))
        self._chime_played = False
        self._last_chime_ts = 0.0
        self._beep_once_guard = False

        # === ADD SPEECH RATE VARIABLE HERE ===
        self.speech_rate_var = tk.IntVar(value=0)

        self._duck_gain = 1.0
        self._duck_active = False

        # === NEW: Vision state initialization ===
        self._last_image_path = None
        self._last_vision_reply = ""
        self._last_was_vision = False
        self._vision_context_until = 0.0
        self._vision_turns_left = 0

        # Muting control
        self.text_ai_muted = False
        self.vision_ai_muted = False

        self._mute_lock = threading.Lock()
        # === Initialize UI components FIRST ===
        self._setup_ui()

        # === Initialize AI engines AFTER UI ===
        self._setup_ai_engines()
        # === ADD THIS LINE ===
        self._load_personalities()  # This will populate the combo box

        # Apply config defaults
        self._apply_config_defaults()

        # Auto-refresh models after UI is ready
        self.master.after(1500, self._refresh_models)  # 1.5 second delay


    def _setup_ui(self):
        from tkinter import ttk

        """Initialize all UI components"""
        # Top controls
        top = tk.ttk.Frame(self.master)
        top.grid(row=0, column=0, columnspan=12, sticky="we")

        self.light = tk.Canvas(top, width=48, height=48, highlightthickness=0)
        self.circle = self.light.create_oval(4, 4, 44, 44, fill="#f1c40f", outline="")
        self.light.grid(row=0, column=0, padx=10, pady=10)

        self.start_btn = ttk.Button(top, text="Start", command=self.start)
        self.stop_btn = ttk.Button(top, text="Stop", command=self.stop, state=tk.DISABLED)

        self.reset_btn = ttk.Button(top, text="Reset Chat", command=self.reset_chat)
        self.reset_btn.grid(row=2, column=6, padx=6, sticky="w")

        self.start_btn.grid(row=0, column=1, padx=6)
        self.stop_btn.grid(row=0, column=2, padx=6)

        # External light indicator
        self.external_light_btn = ttk.Button(top, text="External Light", command=self.toggle_external_light)
        self.external_light_btn.grid(row=2, column=5, padx=6)  # Adjust column as needed

        # STOP SPEAKING button
        self.stop_speech_btn = ttk.Button(top, text="Stop Speaking", command=self.stop_speaking)
        self.stop_speech_btn.grid(row=0, column=3, padx=6)
        # === MUTE CONTROLS ===
        # Add this after your existing buttons in the top row
        mute_frame = ttk.Frame(top)
        mute_frame.grid(row=0, column=14, padx=6, sticky="w")  # Adjust column number as needed

        # Label above the buttons
        ttk.Label(mute_frame, text="Mute:", font=("Segoe UI", 9)).pack(anchor="w")

        # Close Windows button -
        ttk.Button(top, text="Close Windows", command=self.close_all_windows).grid(row=2, column=9, padx=6)

        #==== Setup Load Models =======
        model_frame = ttk.Frame(top)
        model_frame.grid(row=2, column=11, padx=6, sticky="n")

        # Text AI Model
        ttk.Label(model_frame, text="Text AI Model").pack(anchor="n")
        self.text_model_combo = ttk.Combobox(
            model_frame,
            textvariable=self.text_model_var,
            state="readonly",
            width=18
        )
        self.text_model_combo.pack(pady=(2, 2), anchor="n")
        # DON'T set default here - let refresh handle it

        # Vision AI Model
        ttk.Label(model_frame, text="Vision AI Model").pack(anchor="n")
        self.vision_model_combo = ttk.Combobox(
            model_frame,
            textvariable=self.vision_model_var,
            state="readonly",
            width=18
        )
        self.vision_model_combo.pack(pady=(2, 4), anchor="n")
        # DON'T set default here - let refresh handle it



        ttk.Button(model_frame, text="Refresh Models", command=self._refresh_models, width=15).pack(anchor="n")

        # Bind model changes
        self.text_model_combo.bind("<<ComboboxSelected>>", lambda e: self._on_model_change())
        self.vision_model_combo.bind("<<ComboboxSelected>>", lambda e: self._on_model_change())
        # Button container
        mute_buttons_frame = ttk.Frame(mute_frame)
        mute_buttons_frame.pack(fill="x", pady=(2, 0))
        # === PERSONALITY SELECTION ===
        personality_frame = ttk.Frame(top)
        personality_frame.grid(row=2, column=1, padx=10, sticky="w")

        ttk.Label(personality_frame, text="Personality").pack(anchor="w")

        # Personality combo box
        self.personality_var = tk.StringVar(value="Default")
        self.personality_combo = ttk.Combobox(
            personality_frame,
            textvariable=self.personality_var,
            state="readonly",
            width=16,
            values=["Default"]  # We'll populate this later
        )
        self.personality_combo.pack(pady=(2, 0))
        self.personality_combo.bind("<<ComboboxSelected>>", self._on_personality_change)

        # Status label to show current personality
        self.personality_status = ttk.Label(personality_frame, text="‚úì Default", foreground="green")
        self.personality_status.pack()

        # Text AI mute button
        self.text_mute_btn = ttk.Button(
            mute_buttons_frame,
            text="üîá Text",
            width=8,
            command=self.toggle_text_ai_mute
        )
        self.text_mute_btn.pack(side="left", padx=(0, 3))

        # Vision AI mute button
        self.vision_mute_btn = ttk.Button(
            mute_buttons_frame,
            text="üîá Vision",
            width=8,
            command=self.toggle_vision_ai_mute
        )
        self.vision_mute_btn.pack(side="left")

        # Initialize button states
        self.update_mute_buttons()

        # Echo controls
        ttk.Checkbutton(
            top, text="Echo ON", variable=self.echo_enabled_var,
            command=self._sync_echo_state  # Changed from lambda
        ).grid(row=0, column=4, padx=(10, 4))
        ttk.Button(top, text="Show Echo", command=self._toggle_echo_window).grid(row=0, column=5, padx=(4, 10))

        # Images + Pass-to-Text buttons
        _imgbar = ttk.Frame(top)
        _imgbar.grid(row=0, column=6, padx=(6, 4))
        ttk.Button(_imgbar, text="Images‚Ä¶", command=self._toggle_image_window).pack(side="left")
        ttk.Button(_imgbar, text="Pass to Text AI", command=self.pass_vision_to_text).pack(side="left", padx=(6, 0))
        # Manual Refresh
        ttk.Button(_imgbar, text="Refresh Last Reply", command=self._refresh_last_reply).pack(side="left", padx=(6, 0))

        # LaTeX controls
        self.latex_auto = tk.BooleanVar(value=True)
        ttk.Checkbutton(top, text="Auto LaTeX preview", variable=self.latex_auto).grid(row=0, column=7, padx=6)
        ttk.Button(top, text="Show/Hide LaTeX", command=self.toggle_latex).grid(row=0, column=8, padx=6)
        ttk.Button(
            top, text="Copy Raw LaTeX",
            command=lambda: self.latex_win.copy_raw_latex() if hasattr(self, "latex_win") else None
        ).grid(row=0, column=9, padx=(0, 6))

        # === Latex append controls ===
        ttk.Checkbutton(top, text="Append LaTeX", variable=self.latex_append_mode).grid(row=2, column=7, padx=6)
        ttk.Button(top, text="Clear LaTeX", command=self.clear_latex).grid(row=2, column=8, padx=5)

        # Checkbox for speak_math
        self.speak_math_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(top, text="Speak Math", variable=self.speak_math_var,
                        command=self.update_speak_math_setting).grid(row=0, column=12, padx=6)

        # Avatar
        self.avatar_win = None
        self.avatar_kind = tk.StringVar(value="Rings")
        _avatar_bar = ttk.Frame(top)
        _avatar_bar.grid(row=0, column=11, padx=6, sticky="n")
        ttk.Label(_avatar_bar, text="Avatar").pack(anchor="n")
        self.avatar_combo = ttk.Combobox(
            _avatar_bar, textvariable=self.avatar_kind, state="readonly",
            width=14, values=["Rings", "Rectangles", "Rectangles 2", "Radial Pulse", "FaceRadialAvatar", "String Grid","Sphere"]
        )
        self.avatar_combo.current(0)
        self.avatar_combo.pack(pady=(2, 4), anchor="n")
        ttk.Button(_avatar_bar, text="Open/Close", command=self.toggle_avatar).pack(anchor="n")

        def _on_avatar_kind_change(_e=None):
            if self.avatar_win and self.avatar_win.winfo_exists():
                try:
                    self.avatar_win.destroy()
                except Exception:
                    pass
                self.avatar_win = None
                self.open_avatar()

        self.avatar_combo.bind("<<ComboboxSelected>>", _on_avatar_kind_change)

        # Mode selection
        mode_bar = ttk.Frame(top)
        mode_bar.grid(row=0, column=10, padx=6, sticky="n")
        ttk.Label(mode_bar, text="Mode").pack(anchor="n")
        self.mode_combo = ttk.Combobox(
            mode_bar, textvariable=self.duplex_mode, state="readonly", width=18,
            values=["Half-duplex", "Full-duplex (barge-in)"]
        )
        self.mode_combo.current(0)
        self.mode_combo.pack(pady=(2, 0), anchor="n")

        # Ducking UI
        duck = ttk.Frame(top)
        duck.grid(row=1, column=0, columnspan=12, padx=10, sticky="we")
        ttk.Checkbutton(duck, text="Ducking", variable=self.ducking_enable).pack(side="left", padx=(0, 8))
        ttk.Label(duck, text="‚ÜìdB").pack(side="left")
        ttk.Spinbox(duck, from_=0, to=36, width=3, textvariable=self.duck_db).pack(side="left", padx=(2, 8))
        ttk.Label(duck, text="Thr").pack(side="left")
        ttk.Spinbox(duck, from_=200, to=5000, width=5, textvariable=self.duck_thresh).pack(side="left", padx=(2, 8))
        ttk.Label(duck, text="Atk/Rel ms").pack(side="left")
        ttk.Spinbox(duck, from_=5, to=300, width=4, textvariable=self.duck_attack).pack(side="left", padx=(2, 2))
        ttk.Spinbox(duck, from_=20, to=1000, width=5, textvariable=self.duck_release).pack(side="left", padx=(2, 8))
        ttk.Label(duck, text="Gain").pack(side="left", padx=(8, 2))
        self.duck_var = tk.DoubleVar(value=100.0)
        ttk.Progressbar(duck, orient="horizontal", length=120, mode="determinate",
                        variable=self.duck_var, maximum=100.0).pack(side="left", padx=(0, 8))
        self.rms_var = tk.StringVar(value="RMS: 0")
        ttk.Label(duck, textvariable=self.rms_var).pack(side="left")

        # Mic device combo
        ttk.Label(self.master, text="Mic device:").grid(row=2, column=0, sticky="e")
        self.dev_combo = ttk.Combobox(self.master, textvariable=self.device_idx, state="readonly", width=35)
        devs = list_input_devices()
        vals = [f"{i}: {n}" for i, n in devs] if devs else ["No input devices found"]
        self.dev_combo["values"] = vals
        self.dev_combo.current(0)
        self.dev_combo.grid(row=2, column=1, columnspan=9, sticky="w", padx=6, pady=6)

        # Output device combo
        ttk.Label(self.master, text="Speaker device:").grid(row=3, column=0, sticky="e")
        out_vals = self._list_output_devices()
        self.out_combo = ttk.Combobox(self.master, textvariable=self.out_device_idx, state="readonly", width=35,
                                      values=out_vals)
        if out_vals:
            self.out_combo.current(0)
        self.out_combo.grid(row=3, column=1, columnspan=9, sticky="w", padx=6, pady=6)

        # SAPI voices - Enhanced version
        self.sapi_voice_var = tk.StringVar()
        try:
            import pyttsx3
            eng = pyttsx3.init()

            # Get ALL voices first
            all_voices = eng.getProperty("voices")
            self.logln(f"[DEBUG] Found {len(all_voices)} SAPI voices")

            # Create friendly display names with better formatting
            voices_display = []
            self.voice_mapping = {}

            for i, v in enumerate(all_voices):
                # Clean up the voice name for better display
                voice_name = v.name.strip()

                # Optional: Add gender/age indicators if available
                try:
                    gender = getattr(v, 'gender', '')
                    age = getattr(v, 'age', '')
                    if gender or age:
                        voice_name = f"{voice_name} ({gender}{', ' + age if age else ''})"
                except:
                    pass

                voices_display.append(voice_name)
                self.voice_mapping[voice_name] = v.id

            #    self.logln(f"[DEBUG] Voice {i}: '{voice_name}' -> '{v.id}'")

            # Sort voices alphabetically for better UX
            voices_display.sort()

        #    self.logln(f"[DEBUG] Sorted voices list: {voices_display}")

        except Exception as e:
            voices_display = ["(no SAPI5 voices - install pyttsx3)"]
            self.voice_mapping = {}
            self.logln(f"[tts] voice enumeration error: {e}")

        # Create UI elements
        ttk.Label(self.master, text="SAPI Voice:").grid(row=5, column=0, sticky="e", pady=4)

        # Create UI elements
        ttk.Label(self.master, text="SAPI Voice:").grid(row=5, column=0, sticky="e", pady=4)

        self.sapi_combo = ttk.Combobox(
            self.master,
            textvariable=self.sapi_voice_var,
            values=voices_display,
            width=35,
            state="readonly"  # Prevent manual typing
        )
        self.sapi_combo.grid(row=5, column=1, columnspan=5, sticky="w", padx=6, pady=4)

        # Set default selection
        if voices_display and voices_display[0] != "(no SAPI5 voices - install pyttsx3)":
            self.sapi_combo.current(0)
            # Also set the variable explicitly
            self.sapi_voice_var.set(voices_display[0])
            self.logln(f"[tts] Default voice: {voices_display[0]}")

        # === ADD SPEECH SPEED CONTROL HERE ===
        ttk.Label(self.master, text="Speech Speed:").grid(row=6, column=0, sticky="e", pady=4)

        # Slider for precise control
        self.speech_rate_var = tk.IntVar(value=5)
        rate_slider = ttk.Scale(
            self.master,
            from_=-10,
            to=10,
            variable=self.speech_rate_var,
            orient="horizontal",
            length=180
        )
        rate_slider.grid(row=6, column=1, columnspan=3, sticky="we", padx=6, pady=4)

        # Current value display
        self.rate_value_label = ttk.Label(self.master, text="Fast")
        self.rate_value_label.grid(row=6, column=4, sticky="w", padx=5)

        # Preset buttons
        preset_frame = ttk.Frame(self.master)
        preset_frame.grid(row=7, column=1, columnspan=4, sticky="w", pady=2)

        ttk.Button(preset_frame, text="Slow", width=6,
                   command=lambda: self.set_speech_rate(-5)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Normal", width=6,
                   command=lambda: self.set_speech_rate(0)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Fast", width=6,
                   command=lambda: self.set_speech_rate(5)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Reset", width=6,
                   command=lambda: self.set_speech_rate(5)).pack(side="left", padx=2)

        # Initialize the display for fast speed
        self.update_rate_display()

        # Preset buttons
        preset_frame = ttk.Frame(self.master)
        preset_frame.grid(row=7, column=1, columnspan=4, sticky="w", pady=2)

        ttk.Button(preset_frame, text="Slow", width=6,
                   command=lambda: self.set_speech_rate(-5)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Normal", width=6,
                   command=lambda: self.set_speech_rate(0)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Fast", width=6,
                   command=lambda: self.set_speech_rate(5)).pack(side="left", padx=2)
        ttk.Button(preset_frame, text="Reset", width=6,
                   command=lambda: self.set_speech_rate(0)).pack(side="left", padx=2)

        # Text input - KEEP YOUR EXISTING SECTION BUT CHANGE ROW FROM 6 TO 8
        ttk.Label(self.master, text="Text input:").grid(row=8, column=0, sticky="ne", padx=(6, 0), pady=(0, 6))
        self.text_box = ScrolledText(self.master, width=70, height=10, wrap="word")
        self.text_box.grid(row=8, column=1, columnspan=8, sticky="we", padx=6, pady=(0, 6))
        ttk.Button(self.master, text="Send", command=self.send_text).grid(row=8, column=9, sticky="nw", padx=6,
                                                                          pady=(0, 6))
        self.text_box.bind("<Control-Return>", lambda e: (self.send_text(), "break"))

        # Log -
        ttk.Label(self.master, text="Log:").grid(row=9, column=0, sticky="nw", padx=6)
        self.log = tk.Text(self.master, height=12, width=80)
        self.log.grid(row=9, column=1, columnspan=9, sticky="nsew", padx=6)
        self.master.grid_rowconfigure(9, weight=1)  # CHANGE FROM 7 TO 9
        self.master.grid_columnconfigure(9, weight=1)
        self.master.grid_columnconfigure(1, weight=1)
        self.master.grid_columnconfigure(5, weight=1)

        # MULTIPLE LaTeX windows for different contexts
        self.latex_win_text = None  # Main text AI
        self.latex_win_vision = None  # Vision AI
        self.latex_win_search = None  # Search results
        self.latex_win_weather = None  # Weather/other auto-searches

        self._current_latex_context = "text"  # Track which window is active

        # Create the main text window (keep existing behavior)
        DEFAULT_TEXT_PT = int(self.cfg.get("latex_text_pt", 12))
        DEFAULT_MATH_PT = int(self.cfg.get("latex_math_pt", 8))
        DEFAULT_TEXT_FAMILY = self.cfg.get("latex_text_family", "Segoe UI")

        self.latex_win_text = LatexWindow(
            self.master,
            log_fn=self.logln,
            text_family=DEFAULT_TEXT_FAMILY,
            text_size=DEFAULT_TEXT_PT,
            math_pt=DEFAULT_MATH_PT
        )
        self.latex_win_text.title("Text AI - LaTeX Preview")  # Rename it
        # TEMPORARY FIX: Keep old reference
        self.latex_win = self.latex_win_text
        if self.latex_auto.get():
            self.latex_win_text.show()

        # Capture the FIRST SAPI voice for vision
        self._sapi_default_voice_id = None
        try:
            import pyttsx3
            eng = pyttsx3.init()

            # Create friendly display names
            voices_display = []
            self.voice_mapping = {}

            for v in eng.getProperty("voices"):
                display_name = v.name
                voices_display.append(display_name)
                self.voice_mapping[display_name] = v.id

            voices = voices_display

            # Get Text AI voice from JSON config
            config_voice = self.cfg.get("text_ai_voice")

        except Exception as e:
            voices = ["(no SAPI5 voices)"]
            self.voice_mapping = {}
            config_voice = None
            self.logln(f"[tts] voice enumeration error: {e}")

        # ttk.Label(self.master, text="SAPI Voice:").grid(row=5, column=0, sticky="e")
        #  self.sapi_combo = ttk.Combobox(self.master, textvariable=self.sapi_voice_var,
        #       values=voices, width=18)

        # Set combobox to JSON config voice if available
        if config_voice and config_voice in voices:
            self.sapi_combo.set(config_voice)
            self.logln(f"[tts] Text AI voice from config: {config_voice}")
        else:
            self.sapi_combo.current(0)
            if config_voice:
                self.logln(f"[tts] Config voice '{config_voice}' not found, using first available")

    def _setup_ai_engines(self):
        """Initialize AI engines with selected models"""

        # Ensure we have model selections
        if not self.text_model_var.get() or not self.vision_model_var.get():
            self.logln("[ai] ‚ùå No model selections found - refreshing models")
            self._refresh_models()
            if not self.text_model_var.get() or not self.vision_model_var.get():
                self.logln("[ai] ‚ùå Still no model selections - using defaults from config")
                self.text_model_var.set(self.cfg["qwen_model_path"])
                self.vision_model_var.set(self.cfg.get("vl_model") or "qwen2.5-vl:7b")

        # ALWAYS use whatever is selected in the combo boxes
        text_model = self.text_model_var.get()
        vision_model = self.vision_model_var.get()

        print(f"[DEBUG] _setup_ai_engines - Using Text: '{text_model}'")
        print(f"[DEBUG] _setup_ai_engines - Using Vision: '{vision_model}'")# Update UI to show config value

        self.logln(f"[ai] Initializing Text model: {text_model}")
        self.logln(f"[ai] Initializing Vision model: {vision_model}")

        print(f"[DEBUG] _setup_ai_engines - Using Text: '{text_model}'")
        print(f"[DEBUG] _setup_ai_engines - Using Vision: '{vision_model}'")
        # Check if models changed
        if hasattr(self, 'qwen') and self.qwen:
            current_text_model = getattr(self.qwen, 'model_path', 'unknown')
            if current_text_model != text_model:
                print(f"[DEBUG] Text model changed from '{current_text_model}' to '{text_model}'")
        # Initialize ASR (unchanged)
        self.asr = ASR(
            self.cfg["whisper_model"],
            self.cfg["whisper_device"],
            self.cfg["whisper_compute_type"],
            self.cfg["whisper_beam_size"]
        )

        # Initialize Qwen with selected model
        self.qwen = QwenLLM(
            model_path=text_model,  # Use selected model
            model=text_model,
            temperature=self.cfg["qwen_temperature"],
            max_tokens=self.cfg["qwen_max_tokens"]
        )

        # Update vision model
        self.vl_model = vision_model

        # === CRITICAL CONNECTION LINES ===
        # Connect main app to QwenLLM
        self.qwen.set_main_app(self)
        self.logln(
            f"[DEBUG] QwenLLM main_app connected: {hasattr(self.qwen, 'main_app') and self.qwen.main_app is not None}")

        # Connect search handler
        if hasattr(self.qwen, 'set_search_handler'):
            self.qwen.set_search_handler(self.handle_ai_search_request)
            self.logln("[DEBUG] ‚úÖ Search handler connected to QwenLLM")
        else:
            self.logln("[DEBUG] ‚ùå QwenLLM missing set_search_handler method")
        # === END CRITICAL CONNECTION LINES ===

        # === MODIFIED SYSTEM PROMPT - LESS REPETITIVE ===
        # System prompt with REAL-TIME date awareness
        sys_prompt = (
                self.cfg.get("system_prompt")
                or self.cfg.get("qwen_system_prompt")
                or ""
        )

        # Create a dynamic system prompt that explains the AI has real-time awareness
        from datetime import datetime
        current_datetime = datetime.now()
        current_date = current_datetime.strftime("%B %d, %Y")
        current_time = current_datetime.strftime("%I:%M %p")
        current_day = current_datetime.strftime("%A")

        enhanced_system_prompt = f"""{sys_prompt}
    TTS SPEAKING GUIDELINES FOR MATHEMATICS:
    - When transitioning from regular text to equations, add a brief pause
    - After reading equations, add a slight pause before continuing with text
    - Speak mathematical expressions clearly and deliberately
    - For complex equations, break them into logical parts
    - Use phrases like "the equation shows..." or "mathematically speaking..." to introduce formulas
    - when reading a list of instructions or solutions that are numbered, put a pause between the number and the explanation.
    EXAMPLE: 
    1.We do this...  Put a pause after the 1. Make it sound like 1, ...
    2.Do that... put a pause after the 2
    3.Somethings else...put a pause after the 3
    Put pauses in such instances after the numbers.

    ANOTHER EXAMPLE:
    Instead of: "The solution is x=5 and then we continue"
    Use: "The solution is... x equals five... and then we continue"

    Instead of: "We have f(x)=x¬≤+2x+1 which is a parabola"  
    Use: "We have the function... f of x equals x squared plus two x plus one... which describes a parabola"

        BACKGROUND KNOWLEDGE: You have access to real-time information. The current date is {current_date} and the time is {current_time}.
    CURRENT REAL DATE: {current_day}, {current_date} at {current_time}
    *** USE THIS EXACT DATE - DO NOT CALCULATE OR GUESS ***

    CRITICAL RULES:
    1. When asked about the date, ALWAYS use: "{current_day}, {current_date}"
    2. Never calculate days of the week from dates yourself
    3. If your internal knowledge conflicts with the date above, TRUST THE DATE PROVIDED
    4. October 22, 2025 is {current_day} - your training data may be incorrect for this specific date

    USE THIS INFORMATION WHEN:
    - Specifically asked about date, time, or scheduling
    - Questions require current time context
    - Making time-sensitive calculations

    DO NOT:
    - Calculate or guess days of the week
    - Use your internal calendar knowledge for date questions
    - Second-guess the provided current date
        USE THIS INFORMATION WHEN:
        - Specifically asked about date, time, or scheduling
        - Questions require current time context
        - Making time-sensitive calculations

        DO NOT:
        - Include date/time in every response unless specifically relevant
        - Repeat "Today is [date]" in casual conversation
        - Mention your access to real-time info unless asked

        For casual conversation, respond naturally without unnecessary date/time mentions.
        """

        self.qwen.system_prompt = enhanced_system_prompt
        self.logln(f"[qwen] ‚úÖ System prompt updated with balanced time awareness")

        self.logln(f"[qwen] üìÖ Current date in system: {current_date} at {current_time}")


    def _apply_config_defaults(self):
        """Apply configuration defaults to UI"""
        try:
            if self.cfg.get("voice"):
                # No edge voice to set anymore
                pass
            if bool(self.cfg.get("duplex", False)):
                self.duplex_mode.set("Full-duplex (barge-in)")
            else:
                self.duplex_mode.set("Half-duplex")

            if "duck_enable" in self.cfg:
                self.ducking_enable.set(bool(self.cfg.get("duck_enable", True)))
            self.duck_db.set(float(self.cfg.get("duck_db", self.duck_db.get())))
            self.duck_attack.set(int(self.cfg.get("duck_attack_ms", self.duck_attack.get())))
            self.duck_release.set(int(self.cfg.get("duck_release_ms", self.duck_release.get())))
            self.duck_thresh.set(float(self.cfg.get("duck_thresh", self.duck_thresh.get())))
            # === ADD SPEECH RATE CONFIG HERE ===
            if "text_ai_speech_rate" in self.cfg:
                rate = int(self.cfg.get("text_ai_speech_rate"))
                self.speech_rate_var.set(rate)
                self.logln(f"[cfg] Speech rate from config: {rate}")

            self._bargein_enabled = bool(self.cfg.get("bargein_enable", True))
            self._bargein_enabled = self.duplex_mode.get().startswith("Full")

            # === ADD VISION STATE INITIALIZATION HERE ===
            self._vision_turns_left = int(self.cfg.get("vision_followup_max_turns", 5))
            self._vision_context_until = 0.0
            self._last_was_vision = False
            self.logln(f"[vision] initialized: max_turns={self._vision_turns_left}")

        except Exception as e:
            self.logln(f"[cfg] apply defaults error: {e}")

        # Barge-in control
        self._bargein_enabled = bool(self.cfg.get("bargein_enable", True))
        self._barge_latched = False
        self._barge_until = 0.0
        self._barge_cooldown_s = float(self.cfg.get("barge_cooldown_s", 0.7))
        self._barge_min_utt_chars = int(self.cfg.get("barge_min_utt_chars", 3))

        # State
        self.speaking_flag = False
        self.interrupt_flag = False
        self.barge_buffer = None
        self.barge_stream = None
        self.monitor_thread = None
        self._mode_last = None
        self._dev_idx = None

        # Highlight progress fields
        self._tts_total_samples = 0
        self._tts_cursor_samples = 0
        self._hi_stop = True
        self._tts_silent = False
        self._ui_last_ratio = 0.0
        self._ui_eased_ratio = 0.0
        self._ui_gamma = float(self.cfg.get("highlight_gamma", 1.12))

    def update_speak_math_setting(self):
        """Update the speak math setting - can be called when the checkbox changes"""
        self.logln(f"[math] Speak math: {self.speak_math_var.get()}")

    # === NEW: Unified Vision State Manager ===
    def _update_vision_state(self, used_turn: bool = False, reset: bool = False, new_image: bool = False):
        """Thread-safe vision state management"""
        import time as _t

        if reset:
            self._vision_turns_left = 0
            self._vision_context_until = 0.0
            self._last_was_vision = False
            self.logln("[vision] state reset to text mode")
            return

        if new_image:
            # New image = fresh context with full turns
            max_turns = int(self.cfg.get("vision_followup_max_turns", 3))
            self._vision_turns_left = max_turns
            self._vision_context_until = _t.monotonic() + 300  # 5 minutes
            self._last_was_vision = True
            self.logln(f"[vision] new image context: turns={self._vision_turns_left}, window=300s")
            return

        if used_turn and self._vision_turns_left > 0:
            self._vision_turns_left -= 1
            # Always extend context window when using a turn
            self._vision_context_until = _t.monotonic() + 300
            self.logln(f"[vision] used one turn: {self._vision_turns_left} remaining")

    def _should_use_vision_followup(self, text: str) -> bool:
        """Decide if a text turn should reuse the last image."""
        import time as _t

        t = (text or "").strip().lower()
        if not t:
            return False

        now = _t.monotonic()
        has_img = bool(self._last_image_path and os.path.exists(self._last_image_path))

        # Check if we have an active vision context
        has_context = has_img and (now <= float(self._vision_context_until or 0))
        has_turns = has_img and (int(self._vision_turns_left or 0) > 0)

        # Detailed logging
        self.logln(
            f"[vision-route] has_img={has_img}, has_context={has_context}, has_turns={has_turns}, turns_left={self._vision_turns_left}")
        self.logln(
            f"[vision-route] context_until={self._vision_context_until}, now={now}, remaining={self._vision_context_until - now if self._vision_context_until else 0}")

        # If we have an image but context expired, give one more turn
        if has_img and not has_context and self._last_was_vision:
            self.logln("[vision-route] context expired but giving grace turn")
            return True

        if not (has_context and has_turns):
            self.logln(f"[vision-route] context expired: has_context={has_context}, has_turns={has_turns}")
            return False

        # If we have context and turns, be more permissive about what goes to vision
        if self._last_was_vision and len(t) < 100:  # Short follow-ups after vision
            self.logln("[vision-route] short follow-up after vision -> use vision")
            return True

        # Explicit vision cues
        vision_cues = [
            "image", "picture", "photo", "this", "that", "it", "they",
            "what color", "how many", "count", "describe", "explain",
            "on the left", "on the right", "in the middle", "in the background",
            "do you see", "can you see", "find", "locate", "identify"
        ]

        cue_hit = any(cue in t for cue in vision_cues)
        self.logln(f"[vision-route] vision cues matched: {cue_hit}")

        return cue_hit

    # === FIXED: _on_new_image ===
    def _on_new_image(self, path: str):
        """Called by ImageWindow whenever the current image changes (open/snapshot/drag)."""
        try:
            if not path:
                return
            abs_path = os.path.abspath(path)
            if abs_path == self._last_image_path:
                return

            self._last_image_path = abs_path
            # FIXED: Use unified state management
            self._update_vision_state(new_image=True)
            self.logln(f"[vision] context image set -> {os.path.basename(abs_path)}")

        except Exception as e:
            self.logln(f"[vision] _on_new_image error: {e}")

    def _sync_image_context_from_window(self):
        """If the image window already has a file path, sync it into App._last_image_path."""
        try:
            if hasattr(self, "_img_win") and self._img_win and self._img_win.winfo_exists():
                path = getattr(self._img_win, "_img_path", None)
                if path and os.path.isfile(path):
                    abs_path = os.path.abspath(path)
                    if abs_path != self._last_image_path:
                        self._on_new_image(abs_path)
        except Exception:
            pass

    # === FIXED: handle_text_query ===
    def handle_text_query(self, text):
        self.logln(f"[user] {text}")

        # === COMMAND ROUTING SHOULD BE FIRST ===
        # Route camera/image commands first - BEFORE any AI processing
        if self._route_command(text):
            self.logln(f"[DEBUG] Command routed, returning early")
            return

        self._sync_image_context_from_window()

        # Route the user question to appropriate model
        try:
            use_vision = self._should_use_vision_followup(text)
            self.logln(f"[DEBUG] use_vision: {use_vision}")

            if use_vision:
                self.logln(f"[vision] follow-up ‚Üí reuse last image (turns left: {self._vision_turns_left})")
                reply = self._ollama_generate_with_retry(text, images=[self._last_image_path])
                 # Only decrement AFTER successful generation
                self._update_vision_state(used_turn=True)
                self._last_was_vision = True
                # PREVIEW VISION RESPONSE
                self.preview_latex(reply, context="vision")
            else:
                # normal text model path - use search-enhanced generation
                if hasattr(self.qwen, 'generate_with_search'):
                    reply = self.qwen.generate_with_search(text)
                else:
                    reply = self.qwen.generate(text)

                # CLEAN TEXT RESPONSE
                reply = clean_model_output(reply)  # ‚Üê ADD THIS LINE

                # PREVIEW TEXT RESPONSE
                self.preview_latex(reply, context="text")

                # Only reset if we're definitely not in a vision context
                if not self._should_use_vision_followup("dummy"):
                    self._update_vision_state(reset=True)

        except Exception as e:
            self.logln(f"[llm/vision] {e}\n[hint] Is Ollama running?  ollama serve")
            self.set_light("idle")
            return

        self.logln(f"[qwen] {reply}")

        self._set_last_vision_reply(reply)
        clean = clean_for_tts(reply, speak_math=self.speak_math_var.get())

        # unified playback fencing
        with self._play_lock:
            self._play_token += 1
            my_token = self._play_token
            self.interrupt_flag = False
            self.speaking_flag = True

        self.set_light("speaking")
        role = "vision" if use_vision else "text"

        self._latex_theme("vision" if role == "vision" else "default")

        try:
            if self.synthesize_to_wav(clean, self.cfg["out_wav"], role=role):
                # Use the appropriate window for highlighting
                if use_vision:
                    target_win = self.ensure_latex_window("vision")
                else:
                    target_win = self.ensure_latex_window("text")

                play_path = self.cfg["out_wav"]
                if bool(self.echo_enabled_var.get()):
                    try:
                        play_path, _ = self.echo_engine.process_file(self.cfg["out_wav"], "out/last_reply_echo.wav")
                        self.logln("[echo] processed -> out/last_reply_echo.wav")
                    except Exception as e:
                        self.logln(f"[echo] processing failed: {e} (playing dry)")
                self.play_wav_with_interrupt(play_path, token=my_token)
        finally:
            self.speaking_flag = False
            self.interrupt_flag = False
            self.set_light("idle")
            self._latex_theme("default")

            # end query

    def mute_text_ai(self):
        """Mute Text AI audio output - prevents Text AI TTS"""
        with self._mute_lock:
            self.text_ai_muted = True
            self.logln("[mute] üîá Text AI audio muted - will not speak")
            self.update_mute_buttons()

    def unmute_text_ai(self):
        """Unmute Text AI audio output - allows Text AI TTS"""
        with self._mute_lock:
            self.text_ai_muted = False
            self.logln("[mute] üîä Text AI audio unmuted - can speak again")
            self.update_mute_buttons()

    def mute_vision_ai(self):
        """Mute Vision AI audio output - prevents Vision AI TTS"""
        with self._mute_lock:
            self.vision_ai_muted = True
            self.logln("[mute] üîá Vision AI audio muted - will not speak")
            self.update_mute_buttons()

    def unmute_vision_ai(self):
        """Unmute Vision AI audio output - allows Vision AI TTS"""
        with self._mute_lock:
            self.vision_ai_muted = False
            self.logln("[mute] üîä Vision AI audio unmuted - can speak again")
            self.update_mute_buttons()

    def toggle_text_ai_mute(self):
        """Toggle Text AI mute state - button command"""
        with self._mute_lock:
            self.text_ai_muted = not self.text_ai_muted
            state = "muted" if self.text_ai_muted else "unmuted"
            self.logln(f"[mute] {'üîá' if self.text_ai_muted else 'üîä'} Text AI {state}")
            self.update_mute_buttons()

    def update_mute_buttons(self):
        """Update mute button appearance based on current mute state"""
        try:
            # Update Text AI button - shows OPPOSITE state (click mute icon to mute)
            if self.text_ai_muted:
                self.text_mute_btn.config(text="üîä Text")  # Shows UNMUTE symbol (currently muted)
            else:
                self.text_mute_btn.config(text="üîá Text")  # Shows MUTE symbol (currently unmuted)

            # Update Vision AI button - shows OPPOSITE state
            if self.vision_ai_muted:
                self.vision_mute_btn.config(text="üîä Vision")  # Shows UNMUTE symbol (currently muted)
            else:
                self.vision_mute_btn.config(text="üîá Vision")  # Shows MUTE symbol (currently unmuted)

        except Exception as e:
            # Fail silently if buttons don't exist yet (during initialization)
            pass

    def temporary_mute_for_speech(self, speaking_ai):
        """
        Temporarily mute the other AI only during speech - IMPROVED VERSION
        """
        with self._speaker_lock:
            # Store who is currently speaking
            self._current_speaker = speaking_ai

            # Use a small delay to ensure the mute happens before speech starts
            def apply_mute():
                with self._speaker_lock:
                    # Mute the opposite AI
                    if speaking_ai == "text":
                        if not self.vision_ai_muted:
                            self.vision_ai_muted = True
                            self.logln(f"[mute] üîá Temporarily muted Vision AI (Text AI is speaking)")
                            self.update_mute_buttons()
                    elif speaking_ai == "vision":
                        if not self.text_ai_muted:
                            self.text_ai_muted = True
                            self.logln(f"[mute] üîá Temporarily muted Text AI (Vision AI is speaking)")
                            self.update_mute_buttons()

            # Apply the mute after a brief delay to ensure it happens before audio playback
            self.master.after(50, apply_mute)

    def unmute_after_speech(self):
        """
        Unmute the other AI after speech finishes - IMPROVED VERSION
        """

        def apply_unmute():
            with self._speaker_lock:
                if self._current_speaker:
                    # Unmute the opposite AI
                    if self._current_speaker == "text":
                        if self.vision_ai_muted:
                            self.vision_ai_muted = False
                            self.logln(f"[mute] üîä Unmuted Vision AI (Text AI finished speaking)")
                    elif self._current_speaker == "vision":
                        if self.text_ai_muted:
                            self.text_ai_muted = False
                            self.logln(f"[mute] üîä Unmuted Text AI (Vision AI finished speaking)")

                    self._current_speaker = None
                    self.update_mute_buttons()

        # Apply the unmute after a brief delay to ensure speech is completely finished
        self.master.after(100, apply_unmute)

    def toggle_vision_ai_mute(self):
        """Toggle Vision AI mute state - button command"""
        with self._mute_lock:
            self.vision_ai_muted = not self.vision_ai_muted
            state = "muted" if self.vision_ai_muted else "unmuted"
            self.logln(f"[mute] {'üîá' if self.vision_ai_muted else 'üîä'} Vision AI {state}")
            self.update_mute_buttons()

    def clear_latex(self):
        """Clear all LaTeX windows - called by Clear LaTeX button"""
        try:
            # Clear the main text window
            if self.latex_win_text and self.latex_win_text.winfo_exists():
                self.latex_win_text.clear()
                self.logln("[latex] Text window cleared")

            # Clear vision window if it exists
            if self.latex_win_vision and self.latex_win_vision.winfo_exists():
                self.latex_win_vision.clear()
                self.logln("[latex] Vision window cleared")

            # Clear search window if it exists
            if self.latex_win_search and self.latex_win_search.winfo_exists():
                self.latex_win_search.clear()
                self.logln("[latex] Search window cleared")

        except Exception as e:
            self.logln(f"[latex] Clear error: {e}")

    # === FIXED: ask_vision ===
    def ask_vision(self, image_path: str, prompt: str):
        """Called by ImageWindow when the user presses 'Ask model'."""

        # remember the most recent image explicitly
        self._last_image_path = image_path
        self._update_vision_state(new_image=True)

        def _worker():
            try:
                self.logln(f"[vision] {os.path.basename(image_path)} | prompt: {prompt}")
                self.preview_latex(prompt, context="vision")
                reply = self._ollama_generate_with_retry(prompt, images=[image_path])

                # FIXED: Use unified state management
                self._update_vision_state(used_turn=True)

                self.logln(f"[qwen] {reply}")
                self.preview_latex(reply, context="vision")

                # CRITICAL FIX: Store the reply IMMEDIATELY after generation
                self._set_last_vision_reply(reply, source="ask_vision")

                clean = clean_for_tts(reply, speak_math=self.speak_math_var.get())

                with self._play_lock:
                    self._play_token += 1
                    my_token = self._play_token
                    self.interrupt_flag = False
                    self.speaking_flag = True

                self.set_light("speaking")

                self._latex_theme("vision")

                try:
                    if self.synthesize_to_wav(clean, self.cfg["out_wav"], role="vision"):
                        # Use VISION window for highlighting
                        vision_win = self.ensure_latex_window("vision")
                        self.master.after(0, vision_win._prepare_word_spans)
                        play_path = self.cfg["out_wav"]
                        if bool(self.echo_enabled_var.get()):
                            try:
                                play_path, _ = self.echo_engine.process_file(self.cfg["out_wav"],
                                                                             "out/last_reply_echo.wav")
                                self.logln("[echo] processed -> out/last_reply_echo.wav")
                            except Exception as e:
                                self.logln(f"[echo] processing failed: {e} (playing dry)")
                        self.play_wav_with_interrupt(play_path, token=my_token)
                finally:
                    self.set_light("idle")
                    self._latex_theme("default")

            except Exception as e:
                self.logln(f"[vision] error: {e}")
                self.set_light("idle")

        self.set_light("listening")
        threading.Thread(target=_worker, daemon=True).start()

    def handle_ai_search_request(self, search_query: str) -> str:
        """Handle search requests from the AI using the existing web search system"""
        self._last_search_query = search_query
        self.logln(f"[AI Search] Query: {search_query}")

        self.start_search_progress_indicator()

        try:
            # Use your existing brave_search method WITH THE ORIGINAL QUERY
            results = self.brave_search(search_query, 6)
            self.logln(f"[DEBUG] Brave API returned {len(results)} results")

            search_summary = f"Search results for: {search_query}\n\n"

            # Process results using your existing methods
            for i, item in enumerate(results, 1):
                self.logln(f"[DEBUG] Processing result {i}: {item.title}")
                self.logln(f"[DEBUG] URL: {item.url}")

                try:
                    html = self.polite_fetch(item.url)
                    self.logln(f"[DEBUG] HTML fetched: {len(html) if html else 'FAILED'} chars")

                    if html:
                        # Use your existing text extraction
                        text = self.extract_readable(html, item.url)
                        self.logln(f"[DEBUG] Readable text extracted: {len(text)} chars")

                        if len(text) > 400:  # Only summarize if we got substantial content
                            # USE THE ENHANCED SUMMARIZATION (NOW WITH QUERY CONTEXT)
                            self.logln(f"[DEBUG] Calling summarise_for_ai_search...")
                            summary = self.summarise_for_ai_search(text[:12000], item.url, None)
                            self.logln(f"[DEBUG] Summary generated: {len(summary)} chars")

                            search_summary += f"## Result {i}: {item.title}\n"
                            search_summary += f"URL: {item.url}\n"
                            search_summary += f"Summary: {summary}\n\n"
                        else:
                            self.logln(f"[DEBUG] Text too short for summarization: {len(text)} chars")
                            search_summary += f"## Result {i}: {item.title}\n"
                            search_summary += f"URL: {item.url}\n"
                            search_summary += f"Snippet: {item.snippet}\n\n"
                    else:
                        self.logln(f"[DEBUG] HTML fetch failed")
                        search_summary += f"## Result {i}: {item.title}\n"
                        search_summary += f"URL: {item.url}\n"
                        search_summary += f"Snippet: {item.snippet}\n\n"

                except Exception as e:
                    self.logln(f"[DEBUG] Error processing result {i}: {e}")
                    search_summary += f"## Result {i}: {item.title}\n"
                    search_summary += f"URL: {item.url}\n"
                    search_summary += f"Error processing: {str(e)}\n\n"
                    continue

            self.logln(f"[DEBUG] Final search summary: {len(search_summary)} chars")
            self.stop_search_progress_indicator()
            return search_summary

        except Exception as e:
            self.logln(f"[DEBUG] Search failed completely: {e}")
            self.stop_search_progress_indicator()
            return f"Search failed: {str(e)}"

        except Exception as e:
            # ===  STOP PROGRESS INDICATOR ON ERROR ===
            self.stop_search_progress_indicator()
            return f"Search failed: {str(e)}"

    def _process_ai_response(self, response: str, from_search_method: bool = False) -> str:
        """
        Process AI response - don't remove search markers when called from generate_with_search
        """
        import re

        # Look for search commands in the response
        search_pattern = r'\[SEARCH:\s*(.*?)\]'
        searches = re.findall(search_pattern, response, re.IGNORECASE)

        if searches:
            self.logln(f"[AI] Detected {len(searches)} search request(s)")

            # Only remove search markers if NOT called from generate_with_search
            if not from_search_method:
                for search_query in searches:
                    clean_query = search_query.strip()
                    response = response.replace(f"[SEARCH: {search_query}]",
                                                f"[I'm searching for: {clean_query}]")
            else:
                # If called from generate_with_search, keep the markers so it can process them
                self.logln(f"[AI] Preserving search markers for generate_with_search: {searches}")

        return response

    # === ENHANCED: pass_vision_to_text ===
    def pass_vision_to_text(self):
        """
        If a vision reply exists, move it into the Text input box and AUTO-SEND.
        """
        try:
            vision_reply = (getattr(self, '_last_vision_reply', "") or "").strip()
            if not vision_reply:
                self.logln("[pass] nothing to pass from vision")
                return

            # Reset vision state
            self._update_vision_state(reset=True)

            # Create formatted message
            header = "The vision AI provided this information about an image:"
            formatted_message = f"{header}\n\n{vision_reply}\n\nPlease analyze this information, solve any problems mentioned, and discuss as necessary."

            # Move to text box
            self.text_box.delete("1.0", "end")
            self.text_box.insert("1.0", formatted_message)
            self.text_box.focus_set()
            self.text_box.see("end")

            self.logln(f"[pass] vision information sent to Text AI - AUTO-SENDING")

            # AUTO-SEND after a brief delay
            self.master.after(500, self.auto_send_text)

        except Exception as e:
            self.logln(f"[pass] error: {e}")

    def auto_send_text(self):
        """Automatically send the current text box content"""
        try:
            text = self.text_box.get("1.0", "end-1c").strip()
            if text:
                self.logln("[auto-send] Sending to Text AI...")
                # Clear the text box to show it's being processed
                self.text_box.delete("1.0", "end")
                # Process the query
                threading.Thread(target=self.handle_text_query, args=(text,), daemon=True).start()
            else:
                self.logln("[auto-send] No text to send")
        except Exception as e:
            self.logln(f"[auto-send] error: {e}")

    def _set_last_vision_reply(self, reply: str, source: str = "unknown"):
        """Store the most recent vision reply with better tracking."""
        try:
            # Use thread-safe assignment
            self._last_vision_reply = (reply or "").strip()

            # short preview for visibility in your Log panel
            preview = self._last_vision_reply.strip().replace("\n", " ")
            if len(preview) > 120:
                preview = preview[:117] + "..."

            # Enhanced logging with source tracking
            self.logln(f"[vision][cache] ‚úÖ {source}: saved reply ({len(self._last_vision_reply)} chars): {preview}")

            # Debug info
            self.logln(f"[vision][cache] turns_left={self._vision_turns_left}, last_was_vision={self._last_was_vision}")

        except Exception as e:
            self.logln(f"[vision][cache] ‚ùå {source}: failed to store reply: {e}")

    def _refresh_last_reply(self):
        """Debug method to show what's currently in the last vision reply."""
        try:
            txt = (getattr(self, '_last_vision_reply', "") or "").strip()
            if txt:
                self.logln(f"[refresh] Last vision reply exists: {len(txt)} chars")
                preview = txt[:100] + "..." if len(txt) > 100 else txt
                self.logln(f"[refresh] Preview: {preview}")
            else:
                self.logln("[refresh] No last vision reply found")

            # Also show vision state
            self.logln(
                f"[refresh] Vision state: turns_left={self._vision_turns_left}, last_was_vision={self._last_was_vision}")

        except Exception as e:
            self.logln(f"[refresh] Error: {e}")

    def _wait_for_file_unlock(self, filepath, timeout=5.0):
        """Wait for a file to be unlocked by another process."""
        import time
        start_time = time.time()
        while time.time() - start_time < timeout:
            try:
                # Try to open the file in exclusive mode
                with open(filepath, 'rb'):
                    return True  # File is unlocked
            except (IOError, OSError, PermissionError):
                # File is still locked, wait a bit
                time.sleep(0.1)
        return False  # Timeout reached

    def _force_close_file_handles(self, filepath):
        """Attempt to force close any processes using the file (Windows only)"""
        try:
            if os.name == 'nt':  # Windows
                import subprocess
                # Use handle.exe from Sysinternals to close file handles
                result = subprocess.run(['handle.exe', filepath],
                                        capture_output=True, text=True, timeout=5)
                if 'No matching handles found' not in result.stdout:
                    self.logln(f"[filelock] found handles to {filepath}, attempting to close")
                    # Could add logic to parse and close handles here if needed
            # For other OS, we rely on the wait method
        except Exception as e:
            self.logln(f"[filelock] force close attempt failed: {e}")

    def debug_vision_state(self):
        """Debug method to show current vision state"""
        import time as _t
        now = _t.monotonic()
        context_valid = now <= float(self._vision_context_until or 0)

        self.logln(f"[vision-debug]")
        self.logln(f"  last_image: {os.path.basename(self._last_image_path) if self._last_image_path else 'None'}")
        self.logln(f"  turns_left: {self._vision_turns_left}")
        self.logln(f"  context_until: {self._vision_context_until} (valid: {context_valid})")
        self.logln(f"  last_was_vision: {self._last_was_vision}")
        self.logln(f"  current_time: {now}")

    def _pass_vision_to_text_voice(self):
        """
        Voice-activated version with auto-send.
        """
        try:
            vision_reply = (getattr(self, '_last_vision_reply', "") or "").strip()
            if not vision_reply:
                self.logln("[send-to-text] No recent vision reply to send.")
                self.play_chime(freq=440, ms=200, vol=0.2)
                return

            # Reset vision state
            self._update_vision_state(reset=True)

            # Create formatted message
            header = "The vision AI provided this information about an image:"
            formatted_message = f"{header}\n\n{vision_reply}\n\nPlease analyze this information, solve any problems mentioned, and discuss as necessary."

            # Move to text box
            self.text_box.delete("1.0", "end")
            self.text_box.insert("1.0", formatted_message)
            self.text_box.focus_set()
            self.text_box.see("end")

            self.logln(f"[send-to-text] Vision information sent to Text AI - AUTO-SENDING")

            # Play confirmation tone
            self.play_chime(freq=880, ms=150, vol=0.15)

            # AUTO-SEND after a brief delay
            self.master.after(800, self.auto_send_text)  # Slightly longer delay for voice

        except Exception as e:
            self.logln(f"[send-to-text] error: {e}")

    # === Core Application Methods ===
    def ensure_latex_window(self, context="text"):
        """Get or create the appropriate LaTeX window for each context"""
        if context == "text":
            if self.latex_win_text is None or not self.latex_win_text.winfo_exists():
                DEFAULT_TEXT_PT = int(self.cfg.get("latex_text_pt", 12))
                DEFAULT_MATH_PT = int(self.cfg.get("latex_math_pt", 8))
                DEFAULT_TEXT_FAMILY = self.cfg.get("latex_text_family", "Segoe UI")

                self.latex_win_text = LatexWindow(
                    self.master, log_fn=self.logln,
                    text_family=DEFAULT_TEXT_FAMILY, text_size=DEFAULT_TEXT_PT, math_pt=DEFAULT_MATH_PT
                )
                self.latex_win_text.title("Text AI - LaTeX Preview")
            return self.latex_win_text

        elif context == "vision":
            if self.latex_win_vision is None or not self.latex_win_vision.winfo_exists():
                DEFAULT_TEXT_PT = int(self.cfg.get("latex_text_pt", 12))
                DEFAULT_MATH_PT = int(self.cfg.get("latex_math_pt", 8))
                DEFAULT_TEXT_FAMILY = self.cfg.get("latex_text_family", "Segoe UI")

                self.latex_win_vision = LatexWindow(
                    self.master, log_fn=self.logln,
                    text_family=DEFAULT_TEXT_FAMILY, text_size=DEFAULT_TEXT_PT, math_pt=DEFAULT_MATH_PT
                )
                self.latex_win_vision.title("Vision AI - LaTeX Preview")
                self.latex_win_vision.set_scheme("vision")  # Blue theme
            return self.latex_win_vision

        elif context == "search":
            if self.latex_win_search is None or not self.latex_win_search.winfo_exists():
                DEFAULT_TEXT_PT = int(self.cfg.get("latex_text_pt", 12))
                DEFAULT_MATH_PT = int(self.cfg.get("latex_math_pt", 8))
                DEFAULT_TEXT_FAMILY = self.cfg.get("latex_text_family", "Segoe UI")

                self.latex_win_search = LatexWindow(
                    self.master, log_fn=self.logln,
                    text_family=DEFAULT_TEXT_FAMILY, text_size=DEFAULT_TEXT_PT, math_pt=DEFAULT_MATH_PT
                )
                self.latex_win_search.title("Search Results - LaTeX Preview")
            return self.latex_win_search

    def start(self):
        if self.running: return
        self.running = True
        self.start_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.set_light("idle")

        # ADD THIS LINE - sync echo state when starting
        self._sync_echo_state()

        threading.Thread(target=self.loop, daemon=True).start()

    def stop(self):
        self.running = False
        self.stop_speaking()
        self.start_btn.config(state=tk.NORMAL)
        self.stop_btn.config(state=tk.DISABLED)
        self.set_light("idle")

        # Close status light when stopping
        self.close_status_light()

        try:
            if self.barge_stream and self.barge_stream.active:
                self.barge_stream.stop()
        except Exception:
            pass
        try:
            if self.barge_stream:
                self.barge_stream.close()
        except Exception:
            pass
        self.barge_stream = None

    def play_search_results(self, path: str, token=None):
        """Play search results audio with proper interrupt support"""
        try:
            # Apply echo effect if enabled
            play_path = path
            if bool(self.echo_enabled_var.get()):
                try:
                    echo_path = "out/search_results_echo.wav"
                    play_path, _ = self.echo_engine.process_file(path, echo_path)
                    self.logln("[echo] processed search results -> out/search_results_echo.wav")
                except Exception as e:
                    self.logln(f"[echo] processing failed: {e} (playing dry)")

            # Use the existing playback infrastructure with token support
            with self._play_lock:
                self._play_token += 1
                my_token = self._play_token
                self.interrupt_flag = False
                self.speaking_flag = True

            self.set_light("speaking")
            self.play_wav_with_interrupt(play_path, token=my_token)

        except Exception as e:
            self.logln(f"[search][playback] Error: {e}")
        finally:
            self.speaking_flag = False
            self.interrupt_flag = False
            self.set_light("idle")

    def reset_chat(self):
        self.qwen.clear_history()
        self.qwen.system_prompt = self.cfg.get("system_prompt", "")
        self.logln("[info] Chat reset.")

    def what_do_you_see_ui(self):
        """Voice command: 'what do you see' -> open camera, take picture, and describe automatically."""
        try:
            # Ensure image window exists and is visible
            self._ensure_image_window()
            if self._img_win is None:
                self.logln("[vision] camera UI unavailable")
                return

            # Show and raise the window
            self._img_win.deiconify()
            self._img_win.lift()

            # Start camera if not already running
            if not getattr(self._img_win, '_live_mode', False):
                self._img_win.start_camera()
                self.logln("[vision] camera started for 'what do you see'")
                # Wait a moment for camera to initialize
                time.sleep(1.5)

            # Take snapshot
            saved_path = self._img_win.snapshot()
            if saved_path:
                self.logln(f"[vision] snapshot taken: {saved_path}")

                # Use the special vision prompt that includes the "I am the vision AI" prefix
                vision_prompt = (
                    "You are the vision AI. Begin your response with 'I am the vision AI, I can see the following: ' "
                    "and then describe what you see in the image in clear, detailed terms. "
                    "Focus on the main subjects, actions, and any notable details."
                )

                # Call the vision system with the special prompt
                self.ask_vision(saved_path, vision_prompt)
            else:
                self.logln("[vision] failed to take snapshot for 'what do you see'")

        except Exception as e:
            self.logln(f"[vision] 'what do you see' error: {e}")
#start
    def loop(self):
        dev_choice = self.dev_combo.get()
        dev_idx = int(dev_choice.split(":")[0]) if ":" in dev_choice else None
        self._dev_idx = dev_idx
        self.logln(f"[audio] mic device={dev_idx}")
        if getattr(self, "_barge_latched", False):
            time.sleep(0.06)
        self.barge_stream, self.barge_buffer = self.start_bargein_mic(dev_idx)

        guard_half = (lambda: self.speaking_flag)
        guard_full = (lambda: False)
        echo_guard = guard_half if self.duplex_mode.get().startswith("Half") else guard_full

        use_frame_ms = 20 if self.duplex_mode.get().startswith("Full") else self.cfg["frame_ms"]
        use_vad_thresh = self.cfg.get("vad_threshold_full", 0.005) if self.duplex_mode.get().startswith(
            "Full") else self.cfg.get("vad_threshold", 0.01)

        listener = VADListener(
            self.cfg["sample_rate"], use_frame_ms,
            self.cfg["vad_aggressiveness"], self.cfg["min_utt_ms"],
            self.cfg["max_utt_ms"], self.cfg["silence_hang_ms"],
            dev_idx, use_vad_thresh
        )
        it = listener.listen(echo_guard=echo_guard)
        self._mode_last = self.duplex_mode.get()
        self.logln(f"[mode] start as {self._mode_last}")

        self.monitor_thread = threading.Thread(target=self.monitor_interrupt, daemon=True)
        self.monitor_thread.start()
        self.logln("[info] Listening‚Ä¶")

        while self.running:
            # === MODIFIED SLEEP MODE CHECK ===
            if self.command_router.sleep_mode:
                # Still process audio but ONLY check for wake commands while sleeping
                try:
                    utt = next(it)
                    # Check if we got any audio data (utt is a numpy array)
                    if utt is not None and utt.size > 0:
                        text = self.asr.transcribe(utt, self.cfg["sample_rate"])

                        # === ADD FILTER HERE ===
                        text = self.command_router.filter_whisper_hallucinations(text)
                        if not text:
                            continue
                        if text:
                            text_lower = text.lower()
                            # ONLY respond to wake commands while sleeping
                            if any(cmd in text_lower for cmd in self.command_router.wake_commands):
                                self.command_router.exit_sleep_mode()
                                # Don't process this utterance further
                                continue
                            else:
                                self.logln(f"[sleep] Ignored: '{text}'")
                                # Play gentle "I'm sleeping" beep
                                self.command_router.play_sleep_reminder_beep()
                except StopIteration:
                    break
                except Exception as e:
                    self.logln(f"[sleep] audio error: {e}")

                continue  # Skip normal processing while sleeping

            cur_mode = self.duplex_mode.get()
            if cur_mode != self._mode_last:
                try:
                    guard_half = (lambda: self.speaking_flag)
                    guard_full = (lambda: False)
                    echo_guard = guard_half if cur_mode.startswith("Half") else guard_full
                    use_frame_ms = 20 if cur_mode.startswith("Full") else self.cfg["frame_ms"]
                    use_vad_thresh = (
                        self.cfg.get("vad_threshold_full", 0.005)
                        if cur_mode.startswith("Full")
                        else self.cfg.get("vad_threshold", 0.01)
                    )
                    listener = VADListener(
                        self.cfg["sample_rate"], use_frame_ms,
                        self.cfg["vad_aggressiveness"], self.cfg["min_utt_ms"],
                        self.cfg["max_utt_ms"], self.cfg["silence_hang_ms"],
                        self._dev_idx, use_vad_thresh
                    )
                    it = listener.listen(echo_guard=echo_guard)
                    self._mode_last = cur_mode
                    self.logln(f"[mode] switched to {cur_mode} (frame_ms={use_frame_ms}, vad_thresh={use_vad_thresh})")
                    self._beep_once_guard = False
                    self._bargein_enabled = cur_mode.startswith("Full")

                except Exception as e:
                    self.logln(f"[mode] switch error: {e}")

            if self.speaking_flag and cur_mode.startswith("Half"):
                time.sleep(0.02)
                continue

            self.set_light("listening")
            if cur_mode.startswith("Half") and not self._beep_once_guard:
                self.brief_listen_prompt()
                self._beep_once_guard = True

            try:
                utt = next(it)  # ‚Üê THIS IS THE ONLY utt = next(it) CALL NOW
            except StopIteration:
                break
            if not self.running:
                break
            if self.speaking_flag and self.duplex_mode.get().startswith("Half"):
                continue

            text = self.asr.transcribe(utt, self.cfg["sample_rate"])

            # === ADD FILTER HERE ===
            text = self.command_router.filter_whisper_hallucinations(text)
            if not text:
                continue

            self._sync_image_context_from_window()

            if not text:
                continue
            self.logln(f"[asr] {text}")

            # Route camera/image commands first (spoken commands)
            if self._route_command(text):
                continue
            self._sync_image_context_from_window()
            if getattr(self, "_barge_latched", False):
                if time.monotonic() < getattr(self, "_barge_until", 0.0):
                    if len(text.strip()) < int(self.cfg.get("barge_min_utt_chars", 3)):
                        self.logln("[barge-in] suppressing tiny fragment")
                        continue
                    self.logln("[barge-in] listen-only window: suppressing LLM/TTS")
                    continue
                else:
                    self._barge_latched = False

            try:
                use_vision = self._should_use_vision_followup(text)

                if use_vision:
                    self.logln(f"[vision][voice] follow-up ‚Üí reuse last image (turns left: {self._vision_turns_left})")
                    reply = self._ollama_generate_with_retry(text, images=[self._last_image_path])
                    # Only decrement AFTER successful generation
                    self._update_vision_state(used_turn=True)
                    self._last_was_vision = True  # Ensure this stays True

                    # CRITICAL FIX: Store follow-up replies too
                    self._set_last_vision_reply(reply, source="voice_followup")

                else:
                    # normal text model path
                    # Use search-enhanced generation
                    reply = self.qwen.generate_with_search(text)
                    reply = clean_model_output(reply)
                    # Only reset if we're definitely not in a vision context
                    if not self._should_use_vision_followup("dummy"):  # Check if context expired
                        self._update_vision_state(reset=True)

            except Exception as e:
                self.logln(f"[llm/vision] {e}\n[hint] Is Ollama running?  ollama serve")
                self.set_light("idle")
                continue

            self.logln(f"[qwen] {reply}")
            # PREVIEW THE RESPONSE FOR VOICE QUERIES
            self.preview_latex(reply, context="text")

            clean = clean_for_tts(reply, speak_math=self.speak_math_var.get())
            self.speaking_flag = True
            self.interrupt_flag = False
            self.set_light("speaking")

            role = "vision" if use_vision else "text"

            self._latex_theme("vision" if role == "vision" else "default")

            try:
                if self.synthesize_to_wav(clean, self.cfg["out_wav"], role=role):
                    play_path = self.cfg["out_wav"]
                    if bool(self.echo_enabled_var.get()):
                        try:
                            play_path, _ = self.echo_engine.process_file(self.cfg["out_wav"],
                                                                         "out/last_reply_echo.wav")
                            self.logln("[echo] processed -> out/last_reply_echo.wav")
                        except Exception as e:
                            self.logln(f"[echo] processing failed: {e} (playing dry)")
                    self.play_wav_with_interrupt(play_path)
            finally:
                self.speaking_flag = False
                self.interrupt_flag = False
                self.set_light("idle")

                # Only reset theme if we were using vision theme
                if role == "vision":
                    self._latex_theme("default")

        self.stop()

    # === Voice/Audio Methods ===
    def start_bargein_mic(self, device_idx):
        q = deque(maxlen=64)

        def callback(indata, frames, time_info, status):
            if self.speaking_flag and self._bargein_enabled:
                try:
                    # Validate audio data before appending
                    if indata is not None and indata.size > 0:
                        # Check for NaN or infinite values
                        if not np.any(np.isnan(indata)) and not np.any(np.isinf(indata)):
                            q.append(np.copy(indata))
                except Exception as e:
                    self.logln(f"[bargein_callback] Error: {e}")

        try:
            stream = sd.InputStream(
                device=device_idx, samplerate=self.cfg["sample_rate"],
                channels=1, dtype="float32", blocksize=1024, callback=callback
            )
            stream.start()
            return stream, q
        except Exception as e:
            self.logln(f"[bargein_mic] Failed to start: {e}")
            return None, deque(maxlen=64)

    def monitor_interrupt(self):
        import numpy as _np, time as _time
        threshold_interrupt = self.cfg.get("bargein_threshold", 1500)
        trips_needed = int(self.cfg.get("barge_trips_needed", 3))
        trips = 0
        dt = 0.05

        while self.running:
            if self.speaking_flag and self.barge_buffer and len(self.barge_buffer) > 0:
                try:
                    audio = _np.concatenate(list(self.barge_buffer))
                    self.barge_buffer.clear()

                    if audio.size == 0:
                        _time.sleep(dt)
                        continue

                    # SAFE RMS CALCULATION
                    if audio.size > 0:
                        # Clip to safe range to prevent overflow
                        audio_safe = np.clip(audio, -1.0, 1.0)
                        # Calculate RMS with error handling
                        rms_squared = np.mean(audio_safe ** 2)
                        if rms_squared > 0 and not np.isnan(rms_squared) and not np.isinf(rms_squared):
                            rms = np.sqrt(rms_squared) * 32768
                        else:
                            rms = 0.0
                    else:
                        rms = 0.0

                    self._last_rms = float(rms)

                    # Rest of your existing barge-in logic...
                    speech_start_time = getattr(self, '_speech_start_time', 0)
                    is_early_speech = _time.monotonic() - speech_start_time < 1.5

                    is_vision_followup = (
                            self._last_was_vision and
                            self._vision_turns_left > 0 and
                            getattr(self, '_vision_context_until', 0) > _time.monotonic()
                    )

                    effective_threshold = threshold_interrupt
                    if rms > 800:
                        effective_threshold = max(800, threshold_interrupt - 500)

                    if self._bargein_enabled and not is_early_speech and not is_vision_followup:
                        if rms > effective_threshold:
                            trips += 1
                            if trips >= trips_needed:
                                self.logln(f"[barge-in] RMS={rms:.0f} interrupt -> latch listen-only")
                                self.interrupt_flag = True
                                import time as _t
                                self._barge_latched = True
                                self._barge_until = _t.monotonic() + self._barge_cooldown_s
                                try:
                                    self.speaking_flag = False
                                    self.set_light("listening")
                                except Exception:
                                    pass
                                trips = 0
                        else:
                            trips = max(trips - 1, 0)

                    # Ducking logic (with safe RMS)
                    if self.ducking_enable.get():
                        target = 1.0
                        if rms > float(self.duck_thresh.get()):
                            target = 10 ** (-float(self.duck_db.get()) / 20.0)
                        atk = max(5, int(self.duck_attack.get())) / 1000.0
                        rel = max(20, int(self.duck_release.get())) / 1000.0
                        alpha_atk = min(1.0, dt / atk) if atk > 0 else 1.0
                        alpha_rel = min(1.0, dt / rel) if rel > 0 else 1.0
                        cur = getattr(self, "_duck_gain", 1.0)
                        if target < cur:
                            cur += (target - cur) * alpha_atk
                        else:
                            cur += (target - cur) * alpha_rel
                        self._duck_gain = float(_np.clip(cur, 0.0, 1.0))
                        active_now = self._duck_gain < 0.98
                        if self._duck_log:
                            if active_now and not self._duck_active:
                                self.logln(f"[duck] engage gain={self._duck_gain:.2f} (rms={rms:.0f})")
                            elif not active_now and self._duck_active:
                                self.logln(f"[duck] release (rms={rms:.0f})")
                        self._duck_active = bool(active_now)
                    else:
                        self._duck_gain = 1.0
                        self._duck_active = False

                    self.master.after(0, self._update_duck_ui)

                except Exception as e:
                    self.logln(f"[monitor_interrupt] Error: {e}")
                    # Continue running even if there's an error in one iteration
                    _time.sleep(dt)

            else:
                cur = getattr(self, "_duck_gain", 1.0)
                rel = max(20, int(self.duck_release.get())) / 1000.0 if hasattr(self, "duck_release") else 0.25
                alpha_rel = min(1.0, dt / rel) if rel > 0 else 1.0
                cur += (1.0 - cur) * alpha_rel
                self._duck_gain = float(_np.clip(cur, 0.0, 1.0))
                self._duck_active = False
                self.master.after(0, self._update_duck_ui)
                _time.sleep(dt)

    def _update_duck_ui(self):
        try:
            g = float(getattr(self, '_duck_gain', 1.0))
            self.duck_var.set(100.0 * g)
            self.rms_var.set(f"RMS: {int(getattr(self, '_last_rms', 0))}")
        except Exception:
            pass

    def play_chime(self, freq=880, ms=140, vol=0.20):
        """Play a chime with error handling to avoid file locking issues."""
        try:
            fs = 16000
            n = int(fs * (ms / 1000.0))
            t = np.linspace(0, ms / 1000.0, n, endpoint=False)
            s = np.sin(2 * np.pi * freq * t).astype(np.float32)
            fade = np.linspace(0.0, 1.0, min(16, n), dtype=np.float32)
            s[:fade.size] *= fade
            s[-fade.size:] *= fade[::-1]

            # Get output device with fallback
            try:
                out_dev = self._selected_out_device_index()
                sd.play((vol * s).reshape(-1, 1), fs, blocking=False, device=out_dev)
            except Exception as dev_error:
                # Fallback to default device
                self.logln(f"[beep] device error, using default: {dev_error}")
                sd.play((vol * s).reshape(-1, 1), fs, blocking=False)

        except Exception as e:
            self.logln(f"[beep] {e} - chime skipped")
            # Don't re-raise, this is non-critical

    def play_chime2(self, path="beep.mp3", gain_db=0.0):
        try:
            if not hasattr(self, "_beep_cache") or self._beep_cache.get("path") != path:
                seg = AudioSegment.from_file(path)
                self._beep_cache = {"path": path, "seg": seg}
            else:
                seg = self._beep_cache["seg"]
            if gain_db:
                seg = seg.apply_gain(gain_db)
            samples = np.array(seg.get_array_of_samples())
            if seg.channels > 1:
                samples = samples.reshape((-1, seg.channels))
            else:
                samples = samples.reshape((-1, 1))
            samples = samples.astype(np.float32) / (2 ** (8 * seg.sample_width - 1))
            fade = min(int(0.008 * seg.frame_rate), max(1, samples.shape[0] // 6))
            if fade > 0:
                ramp = np.linspace(0.0, 1.0, fade, dtype=np.float32).reshape(-1, 1)
                samples[:fade] *= ramp
                samples[-fade:] *= ramp[::-1]
            sd.play(samples, seg.frame_rate, blocking=True,
                    device=self._selected_out_device_index())
        except Exception as e:
            self.logln(f"[beep mp3] {e} ‚Äî fallback tone")
            self.play_chime()

    def brief_listen_prompt(self):
        if not self.cfg.get("announce_listening", True):
            return
        prev = self.speaking_flag
        try:
            self.speaking_flag = True
            self.play_chime2("beep.mp3")
        finally:
            self.speaking_flag = prev

    def speak_search_status(self, message="Searching the internet for this information"):
        if not message or not message.strip():
            return

        self.logln(f"[search-status] üîä Processing: {message}")

        # === ONLY START PROGRESS FOR ACTUAL WEB SEARCHES ===
        should_start_progress = (
                "search" in message.lower() and
                "personality" not in message.lower() and
                "activating" not in message.lower() and
                "switching" not in message.lower() and
                "activating" not in message.lower()
        )

        if should_start_progress:
            self.start_search_progress_indicator()
            self.logln(f"[search-status] ‚úÖ Started progress for: {message}")
        else:
            self.logln(f"[search-status] ‚è∏Ô∏è  Skipping progress for: {message}")

        try:
            # Clean the text for TTS
            clean_message = clean_for_tts(message, speak_math=self.speak_math_var.get())
            status_path = "out/search_status.wav"

            if self.synthesize_to_wav(clean_message, status_path, role="text"):
                def play_status():
                    try:
                        time.sleep(0.1)

                        # Apply echo if enabled
                        play_path = status_path
                        if bool(self.echo_enabled_var.get()):
                            try:
                                echo_path = "out/search_status_echo.wav"
                                play_path, _ = self.echo_engine.process_file(status_path, echo_path)
                            except Exception as e:
                                self.logln(f"[search-status] echo processing failed: {e}")

                        # Get the output device
                        out_dev = self._selected_out_device_index()

                        # Load and play the audio file
                        data, fs = sf.read(play_path, dtype="float32")
                        if data.size > 0:
                            sd.play(data, fs, blocking=False, device=out_dev)
                            self.logln(f"[search-status] ‚úÖ Playing: {message}")

                    except Exception as e:
                        self.logln(f"[search-status] play error: {e}")

                threading.Thread(target=play_status, daemon=True).start()

        except Exception as e:
            self.logln(f"[search-status] synthesis error: {e}")

    def play_wav_with_interrupt(self, path, token=None):
        import platform as _plat
        start_time = time.monotonic()
        active_token = token
        # Determine which AI is speaking based on context
        if hasattr(self, '_last_was_vision') and self._last_was_vision:
            speaking_ai = "vision"
        else:
            speaking_ai = "text"

        self.temporary_mute_for_speech(speaking_ai)

        # Track speech start time for barge-in protection
        self._speech_start_time = start_time
        try:
            data, fs = sf.read(path, dtype="float32")
            if data.ndim == 1:
                data = data.reshape(-1, 1)
            if data.size == 0:
                return
            total_samples = int(data.shape[0])
            self._tts_total_samples = total_samples
            self._tts_cursor_samples = 0
            self._hi_stop = False

            target_fs = int(self.cfg.get("force_out_samplerate", fs))
            if target_fs != fs:
                try:
                    n = data.shape[0]
                    t = np.linspace(0.0, 1.0, n, endpoint=False)
                    m = int(np.ceil(n * (target_fs / fs)))
                    ti = np.linspace(0.0, 1.0, m, endpoint=False)
                    chans = data.shape[1]
                    out = []
                    for c in range(chans):
                        out.append(np.interp(ti, t, data[:, c]))
                    data = np.stack(out, axis=1).astype(np.float32)
                    fs = target_fs
                    self.logln(f"[audio] resample -> {fs} Hz for output")
                except Exception as e:
                    self.logln(f"[audio] resample failed, using original fs ({fs}): {e}")

            out_dev = self._selected_out_device_index()
            blocksize = self.cfg.get("out_blocksize", 8192)
            latency_hint = self.cfg.get("out_latency", "high")
            extra = None
            try:
                if _plat.system() == "Windows":
                    extra = sd.WasapiSettings(exclusive=False)
            except Exception:
                extra = None

            SILENCE_THRESH = 1e-4
            SILENCE_MAX_BLOCKS = 20
            cursor = 0

            def run_stream():
                nonlocal cursor, fs, data, blocksize, latency_hint, out_dev, extra
                silent_blocks = 0
                last_cursor_check = -1
                stall_ticks = 0
                STALL_TICKS_MAX = int(self.cfg.get("stall_ticks_max", 120))
                RESUME_FADE_SAMPLES = int(0.01 * fs)
                did_fade = False

                def cb(outdata, frames, *_):
                    if (active_token is not None) and (active_token != self._play_token):
                        outdata[:] = 0
                        raise sd.CallbackStop()

                    if self.interrupt_flag or not self.running:
                        outdata[:] = 0
                        raise sd.CallbackStop()

                    nonlocal cursor, silent_blocks, did_fade
                    if self.interrupt_flag or not self.running:
                        outdata[:] = 0
                        raise sd.CallbackStop()

                    end = min(cursor + frames, data.shape[0])
                    out_frames = end - cursor
                    block = data[cursor:end]
                    avg_abs = 0.0
                    gain = float(np.clip(getattr(self, "_duck_gain", 1.0), 0.0, 1.5))

                    if out_frames > 0:
                        out = block.copy()
                        if not did_fade and cursor == 0:
                            n = min(out.shape[0], RESUME_FADE_SAMPLES)
                            if n > 0:
                                ramp = np.linspace(0.0, 1.0, n, dtype=np.float32).reshape(-1, 1)
                                out[:n] *= ramp
                            did_fade = True
                        outdata[:out_frames] = out * gain
                        avg_abs = float(np.mean(np.abs(block))) if block.size else 0.0
                        if avg_abs < SILENCE_THRESH:
                            silent_blocks += 1
                            if silent_blocks >= SILENCE_MAX_BLOCKS:
                                outdata[out_frames:] = 0
                                raise sd.CallbackStop()
                        else:
                            silent_blocks = 0

                    # self._tts_silent = bool(avg_abs < SILENCE_THRESH)
                    env = min(max(avg_abs * 4.0, 0.0), 1.0) ** 0.6
                    AVATAR_LEVELS = 32
                    level = int(env * (AVATAR_LEVELS - 1) + 1e-6)

                    try:
                        self.master.after(0, self._avatar_set_level_async, level)
                    except Exception:
                        pass

                    if end - cursor < frames:
                        outdata[end - cursor:] = 0
                        raise sd.CallbackStop()

                    cursor = end

                # self._tts_cursor_samples = int(cursor)

                def open_stream(extra_settings, device_idx):
                    return sd.OutputStream(
                        samplerate=fs,
                        channels=data.shape[1],
                        dtype="float32",
                        blocksize=blocksize,
                        latency=latency_hint,
                        callback=cb,
                        device=device_idx,
                        extra_settings=extra_settings,
                    )

                chosen_dev = out_dev
                hostapi_name = self._device_hostapi_name(chosen_dev)
                use_extra = bool(hostapi_name and "WASAPI" in hostapi_name)
                self.logln(
                    f"[audio] open stream dev={chosen_dev} hostapi={hostapi_name or 'default'} "
                    f"extra={'wasapi' if use_extra else 'none'}"
                )

                try:
                    ctx = open_stream(extra if use_extra else None, chosen_dev)
                except Exception:
                    self.logln("[audio] stream open failed with WASAPI; retrying without extras")
                    chosen_dev = None
                    ctx = open_stream(None, chosen_dev)

                with ctx:
                    # self.master.after(0, _ui_progress_tick)
                    while self.running and not self.interrupt_flag and cursor < data.shape[0]:
                        if cursor == last_cursor_check:
                            stall_ticks += 1
                            if stall_ticks >= STALL_TICKS_MAX:
                                return False
                        else:
                            last_cursor_check = cursor
                            stall_ticks = 0
                        time.sleep(0.01)
                return True

            ok = run_stream()
            if not ok:
                remaining = data.shape[0] - cursor
                if remaining < int(0.25 * fs):
                    self.logln("[audio] stalled near end ‚Äî skipping retry")
                else:
                    self.logln("[audio] output stalled ‚Äî retrying with larger buffers (resume)")
                    blocksize = max(int(blocksize) if blocksize else 0, 8192)
                    latency_hint = "high"
                    run_stream()
        except Exception as e:
            self.logln(f"[warn] playback error: {e}")
        finally:
            self.unmute_after_speech()
            self.speaking_flag = False
            self.interrupt_flag = False
            self.set_light("idle")
            try:
                if self.avatar_win and self.avatar_win.winfo_exists():
                    self.avatar_win.set_level(0)
            except Exception:
                pass

            self._beep_once_guard = False
            dur = time.monotonic() - start_time
            self.logln(f"[audio] playback done ({dur:.2f}s)")
#Begin route command
    # Replace the entire _route_command method with:
    def _route_command(self, raw_text: str) -> bool:
        """Delegate command routing to external class"""
        return self.command_router.route_command(raw_text)

    # routine ends here

    # === Awaken and Sleep Methods ===
    def close_all_windows(self):
        """Close all secondary windows except avatar and main window"""
        windows_closed = 0

        # Close status light
        self.close_status_light()

        # List of other windows to close
        windows_to_close = [
            'latex_win_text', 'latex_win_vision', 'latex_win_search', 'latex_win',
            'search_win', '_img_win', '_echo_win'
        ]

        for window_name in windows_to_close:
            window = getattr(self, window_name, None)
            if window and hasattr(window, 'winfo_exists') and window.winfo_exists():
                try:
                    if hasattr(window, 'hide'):
                        window.hide()
                    elif hasattr(window, 'withdraw'):
                        window.withdraw()
                    else:
                        window.iconify()
                    windows_closed += 1
                except Exception as e:
                    self.logln(f"[close] Error closing {window_name}: {e}")

        self.logln(f"[close] Closed {windows_closed} windows (avatar remains open)")
        self.play_chime(freq=660, ms=120, vol=0.15)
        return windows_closed




    def _refresh_models(self):
        """Refresh available models from Ollama and set defaults from config"""
        try:
            models = self._get_available_models()

            if models:
                # Update combo boxes
                self.text_model_combo['values'] = models
                self.vision_model_combo['values'] = models

                # Set defaults from config if not already set
                if not self.text_model_var.get():
                    default_text = self.cfg["qwen_model_path"]
                    if default_text in models:
                        self.text_model_var.set(default_text)
                    elif models:  # Fallback to first available
                        self.text_model_var.set(models[0])

                if not self.vision_model_var.get():
                    default_vision = self.cfg.get("vl_model") or "qwen2.5-vl:7b"
                    if default_vision in models:
                        self.vision_model_var.set(default_vision)
                    elif models:  # Fallback to first available
                        self.vision_model_var.set(models[0])

                self.logln(f"[models] Loaded {len(models)} models")
                self.logln(f"[models] Text: {self.text_model_var.get()}, Vision: {self.vision_model_var.get()}")
            else:
                self.logln("[models] No models found - is Ollama running?")

        except Exception as e:
            self.logln(f"[models] Error refreshing: {e}")

    def _get_available_models(self):
        """Get list of available Ollama models"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=10)
            response.raise_for_status()
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]

            # Debug output
            print(f"[DEBUG] Found {len(models)} models: {models}")

            return sorted(models)  # Sort for better UX

        except Exception as e:
            print(f"[DEBUG] Model fetch error: {e}")
            self.logln(f"[models] Could not fetch models: {e}")
            return []


    def _on_model_change(self):
        """Handle model selection changes - PROPERLY restart engines"""
        text_model = self.text_model_var.get()
        vision_model = self.vision_model_var.get()

        if not text_model or not vision_model:
            return

        self.logln(f"[models] Model change requested - Text: '{text_model}', Vision: '{vision_model}'")

        # Store the new selections in config
        self.cfg["qwen_model_path"] = text_model
        self.vl_model = vision_model

        if self.running:
            response = messagebox.askyesno(
                "Model Change",
                f"Changing models requires restarting the AI engine.\n\n"
                f"New models:\n"
                f"Text: {text_model}\n"
                f"Vision: {vision_model}\n\n"
                f"Stop and restart now?"
            )
            if response:
                was_running = True
                self.stop()
                # Reinitialize with new models
                self._setup_ai_engines()
                self.start()  # Restart if it was running
                self.logln(f"[models] ‚úÖ Models changed and engines restarted")
            else:
                # Revert the combo boxes if user cancels
                self._refresh_models()
        else:
            # If not running, just reinitialize
            self._setup_ai_engines()
            self.logln(f"[models] ‚úÖ Models updated: Text={text_model}, Vision={vision_model}")







    def _ollama_generate(self, prompt: str, images=None):
        """Use selected vision model"""
        if not images:
            return self.qwen.generate(prompt)

        # Use the current selected vision model
        current_vision_model = self.vision_model_var.get() or self.vl_model

        b64_images = []
        for it in images:
            if isinstance(it, str) and os.path.isfile(it):
                with open(it, "rb") as f:
                    b64_images.append(base64.b64encode(f.read()).decode("ascii"))
            else:
                b64_images.append(it)

        vision_prefix = (
            "You can see the attached image. Answer directly and concisely.\n"
            "- If the question is 'how many ...', output a number.\n"
            "- If the question is 'what type ...', name the type/class.\n"
            "- Do not claim you are text-based.\n\n"
        )
        full_prompt = vision_prefix + (prompt or "")

        payload = {
            "model": current_vision_model,  # Use selected model
            "prompt": full_prompt,
            "images": b64_images,
            "system": self.vl_system_prompt,
            "stream": False
        }

        self.logln(f"[vision] model={payload['model']} images={len(b64_images)}")


        self.logln(f"[vision] sys[:120]={payload['system'][:120]!r}")
        self.logln(f"[vision] prm[:120]={payload['prompt'][:120]!r}")

        # Try multiple connection methods
        endpoints = [
            "http://127.0.0.1:11434/api/generate",
            "http://localhost:11434/api/generate"
        ]

        for endpoint in endpoints:
            try:
                self.logln(f"[vision] trying endpoint: {endpoint}")
                r = requests.post(endpoint, json=payload, timeout=60)
                r.raise_for_status()
                response = (r.json().get("response") or "").strip()
                self.logln(f"[vision] ‚úÖ success with {endpoint}")
                return response
            except requests.exceptions.ConnectionError as e:
                self.logln(f"[vision] connection failed to {endpoint}: {e}")
                continue
            except Exception as e:
                self.logln(f"[vision] error with {endpoint}: {e}")
                continue

        # If all endpoints fail
        raise RuntimeError(f"Could not connect to Ollama. Tried: {endpoints}")

    def _ollama_generate_with_retry(self, prompt: str, images=None, max_retries=2):
        """Generate with vision model with retry logic"""
        for attempt in range(max_retries + 1):
            try:
                return self._ollama_generate(prompt, images)
            except Exception as e:
                if attempt < max_retries:
                    self.logln(f"[vision] attempt {attempt + 1} failed: {e}, retrying...")
                    time.sleep(1)  # Brief pause before retry
                else:
                    self.logln(f"[vision] all {max_retries + 1} attempts failed")
                    raise e

    # === UI Helper Methods ===


    def stop_speaking(self):
        try:
            self.interrupt_flag = True
            self.speaking_flag = False
            try:
                sd.stop()
            except Exception:
                pass
            self._hi_stop = True
            self._tts_silent = False
            self._ui_last_ratio = 0.0
            self._ui_eased_ratio = 0.0
            try:
                self.latex_win.clear_highlight()
            except Exception:
                pass
            try:
                if self.avatar_win and self.avatar_win.winfo_exists():
                    self.avatar_win.set_level(0)
            except Exception:
                pass
        finally:
            self.set_light("idle")

    def set_light(self, mode):
        # Track previous mode to detect transitions
        previous_mode = getattr(self, '_previous_light_mode', 'idle')

        color = {"idle": "#f1c40f", "listening": "#2ecc71", "speaking": "#e74c3c"}.get(mode, "#f1c40f")

        # Update main light ONLY if external light is not active
        if self.external_light_win is None or not self.external_light_win.winfo_exists() or self.external_light_win.state() == "withdrawn":
            self.light.itemconfig(self.circle, fill=color)
        else:
            # External light is active, keep main light black
            self.light.itemconfig(self.circle, fill="#000000")

        self.state.set(mode)

        # Update external light if it exists and is visible
        try:
            if (self.external_light_win and
                    self.external_light_win.winfo_exists() and
                    self.external_light_win.state() != "withdrawn"):
                self.external_light_win.set_light(color)
        except Exception as e:
            self.logln(f"[light] color update error: {e}")

        # Play beep when transitioning from any state to listening (green) mode
        if mode == "listening" and previous_mode != "listening":
            self.play_chime(freq=660, ms=120, vol=0.12)
            self.logln("[status] ‚úÖ Ready to receive requests")

        # Store current mode for next comparison
        self._previous_light_mode = mode

    def toggle_external_light(self):
        """Toggle the external light window on/off"""
        try:
            if self.external_light_win is None or not self.external_light_win.winfo_exists():
                self.external_light_win = StatusLightWindow(self.master)
                # Set initial color to match current state
                current_mode = self.state.get()
                color = {"idle": "#f1c40f", "listening": "#2ecc71", "speaking": "#e74c3c"}.get(current_mode, "#f1c40f")
                self.external_light_win.set_light(color)
                self.external_light_win.show()

                # Hide the main light in the main window
                self.light.itemconfig(self.circle, fill="#000000")  # Make main light black
                self.logln("[light] Status light opened - main light hidden")
            else:
                if self.external_light_win.state() == "withdrawn":
                    self.external_light_win.show()
                    # Hide main light
                    self.light.itemconfig(self.circle, fill="#000000")
                    self.logln("[light] Status light shown - main light hidden")
                else:
                    self.external_light_win.hide()
                    # Restore main light
                    current_mode = self.state.get()
                    color = {"idle": "#f1c40f", "listening": "#2ecc71", "speaking": "#e74c3c"}.get(current_mode,
                                                                                                   "#f1c40f")
                    self.light.itemconfig(self.circle, fill=color)
                    self.logln("[light] Status light hidden - main light restored")
        except Exception as e:
            self.logln(f"[light] toggle error: {e}")

    # ==== AUDIBLE PROGRESS SOUND ===
    def start(self):
        if self.running: return
        self.running = True
        self.start_btn.config(state=tk.DISABLED)
        self.stop_btn.config(state=tk.NORMAL)
        self.set_light("idle")

        # === REMOVE auto-refresh ===
        # Let the combo boxes keep whatever values they have

        # ADD THIS LINE - sync echo state when starting
        self._sync_echo_state()

        threading.Thread(target=self.loop, daemon=True).start()

    def start_search_progress_indicator(self):
        """Start the search progress indicator"""
        if not hasattr(self, '_search_in_progress'):
            self._search_in_progress = False
            self._search_progress_count = 0
            self._search_progress_timer = None

        if not self._search_in_progress:
            self._search_in_progress = True
            self._search_progress_count = 0
            self._last_search_progress_time = time.time()
            self._schedule_next_progress_beep()
            self.logln("[search] üîç Starting search progress indicator")

    def stop_search_progress_indicator(self):
        """Stop the progress indicator when search completes"""
        if self._search_in_progress:
            self.logln(f"[search] Stopping progress indicator (was at {self._search_progress_count} beeps)")
        self._search_in_progress = False
        if self._search_progress_timer:
            try:
                self.master.after_cancel(self._search_progress_timer)
                self.logln("[search] Progress timer cancelled")
            except:
                pass
            self._search_progress_timer = None

    def set_external_light_color(self, color):
        """Set the color of the external light"""
        try:
            if (self.external_light_win and
                    self.external_light_win.winfo_exists() and
                    self.external_light_win.state() != "withdrawn"):
                self.external_light_win.set_color(color)
        except Exception as e:
            self.logln(f"[light] color change error: {e}")

    def close_status_light(self):
        """Close the status light window"""
        try:
            if self.status_light_win and self.status_light_win.winfo_exists():
                self.status_light_win.destroy()
                self.status_light_win = None
                self.logln("[light] Status light closed")
        except Exception as e:
            self.logln(f"[light] close error: {e}")

    def _play_search_progress_beep(self):
        """Play a progress beep with variation based on search stage"""
        if not self._search_in_progress:
            return

        # === SAFETY CHECK: Stop if too many beeps ===
        if self._search_progress_count > 50:
            self.logln("[search] Safety stop: too many progress beeps")
            self.stop_search_progress_indicator()
            return

        self._search_progress_count += 1

        # Different beep patterns based on search progress
        if self._search_progress_count == 1:
            # First beep - gentle start
            freq = 440  # A4
            duration = 0.08
            vol = 0.1
        elif self._search_progress_count <= 3:
            # Early progress - slightly higher
            freq = 523  # C5
            duration = 0.08
            vol = 0.12
        elif self._search_progress_count <= 6:
            # Middle stage - ascending pattern
            freq = 587  # D5
            duration = 0.09
            vol = 0.14
        else:
            # Extended search - more urgent but not annoying
            freq = 659  # E5
            duration = 0.1
            vol = 0.16

        try:
            fs = 16000
            t = np.linspace(0, duration, int(fs * duration), endpoint=False)

            # Create a pleasant beep with soft attack/decay
            beep = vol * np.sin(2 * np.pi * freq * t)

            # Smooth fade
            fade = int(0.015 * fs)
            if fade > 0:
                beep[:fade] *= np.linspace(0, 1, fade)
                beep[-fade:] *= np.linspace(1, 0, fade)

            # Play non-blocking
            out_dev = self._selected_out_device_index()
            try:
                sd.play(beep, fs, blocking=False, device=out_dev)
            except Exception:
                sd.play(beep, fs, blocking=False)

            self.logln(f"[search] Progress indicator #{self._search_progress_count}")

        except Exception as e:
            self.logln(f"[search] Progress beep error: {e}")

    def _schedule_next_progress_beep(self):
        """Schedule the next progress beep with variable timing"""
        if not self._search_in_progress:
            return

        # Variable timing: more frequent as search takes longer
        if self._search_progress_count <= 3:
            interval = 8000  # 8 seconds for first few beeps
        elif self._search_progress_count <= 6:
            interval = 6000  # 6 seconds for middle stage
        else:
            interval = 5000  # 5 seconds for extended searches

        self._search_progress_timer = self.master.after(interval, self._progress_beep_sequence)

    def _progress_beep_sequence(self):
        """The actual sequence called by the timer"""
        if self._search_in_progress:
            self._play_search_progress_beep()
            self._schedule_next_progress_beep()

    # === END SEARCH PROGRESS METHODS ===

    def _toggle_echo_window(self):
        try:
            if self._echo_win is None or not self._echo_win.winfo_exists():
                self._echo_win = EchoWindow(self.master, self.echo_engine)
            if self._echo_win.state() == "withdrawn":
                self._echo_win.deiconify()
                self._echo_win.lift()
            else:
                self._echo_win.withdraw()
        except Exception as e:
            self.logln(f"[echo] window error: {e}")

    # Add this method to ensure echo state is consistent
    def _sync_echo_state(self):
        """Sync the echo engine state with the UI checkbox"""
        self.echo_engine.enabled = bool(self.echo_enabled_var.get())
        self.logln(f"[echo] state synced: {self.echo_engine.enabled}")

    def _toggle_image_window(self):
        try:
            if not hasattr(self, "_img_win") or self._img_win is None or not self._img_win.winfo_exists():
                # Use the proper ImageWindow class
                self._img_win = self.ImageWindow(
                    self.master,
                    on_send=self.ask_vision,
                    on_image_change=self._on_new_image
                )

            if self._img_win.state() == "withdrawn":
                self._img_win.deiconify()
                self._img_win.lift()
            else:
                self._img_win.withdraw()
        except Exception as e:
            self.logln(f"[image] window error: {e}")

    # ImageWindow class definition
    class ImageWindow(tk.Toplevel):
        """
        Vision helper:
          - Open image (file dialog)
          - Drag & drop (if tkinterdnd2 is installed)
          - Camera preview + snapshot (if opencv is installed)
          - Send to model (calls parent App.ask_vision)
        """

        def __init__(self, master, on_send, on_image_change=None):
            super().__init__(master)
            self.title("Image / Camera")
            self.geometry("720x560")
            self.protocol("WM_DELETE_WINDOW", self.withdraw)

            self._on_send = on_send  # callback: on_send(image_path, prompt)
            self._on_image_change = on_image_change  # notify app when image changes
            self._img_path = None
            self._img_tk = None
            self._cam = None
            self._cam_timer = None
            self._live_mode = False

            # UI
            wrap = ttk.Frame(self)
            wrap.pack(fill="both", expand=True, padx=8, pady=8)
            top = ttk.Frame(wrap)
            top.pack(fill="x")

            ttk.Button(top, text="Open Image‚Ä¶", command=self.open_image).pack(side="left", padx=(0, 6))
            ttk.Button(top, text="Start Camera", command=self.start_camera).pack(side="left", padx=(0, 6))
            ttk.Button(top, text="Stop Camera", command=self.stop_camera).pack(side="left", padx=(0, 6))
            ttk.Button(top, text="Snapshot", command=self.snapshot).pack(side="left", padx=(0, 6))

            ttk.Label(top, text="Prompt:").pack(side="left", padx=(16, 4))
            self.prompt_var = tk.StringVar(value="Please solve/describe any equations in this image. Use LaTeX.")
            self.prompt_entry = ttk.Entry(top, textvariable=self.prompt_var, width=48)
            self.prompt_entry.pack(side="left", fill="x", expand=True)

            ttk.Button(top, text="Ask model", command=self.send_now).pack(side="left", padx=(6, 0))

            self.canvas = tk.Canvas(wrap, bg="#111", highlightthickness=0)
            self.canvas.pack(fill="both", expand=True, pady=(8, 0))
            self.canvas.bind("<Configure>", lambda e: self._redraw())

            # Drag & drop (optional)
            if DND_FILES:
                try:
                    self.drop_target_register(DND_FILES)
                    self.dnd_bind("<<Drop>>", self._on_drop)
                except Exception:
                    pass

        # ---- File ops ----
        def open_image(self):
            from tkinter import filedialog
            path = filedialog.askopenfilename(
                title="Open image",
                filetypes=[("Image files", "*.png;*.jpg;*.jpeg;*.bmp;*.webp;*.tif;*.tiff")]
            )
            if path:
                self.set_image(path)

        def _on_drop(self, event):
            # Windows sends a quoted path; handle multiple too
            paths = self._parse_drop_paths(event.data)
            if paths:
                self.set_image(paths[0])

        @staticmethod
        def _parse_drop_paths(data):
            # minimal parser for common DND formats
            items = []
            cur = ""
            in_quote = False
            for ch in data:
                if ch == '"':
                    in_quote = not in_quote
                elif ch in (" ", "\n") and not in_quote:
                    if cur.strip():
                        items.append(cur.strip('"'))
                    cur = ""
                else:
                    cur += ch
            if cur.strip():
                items.append(cur.strip('"'))
            return items

        def set_image(self, path):
            try:
                img = Image.open(path).convert("RGB")
            except Exception as e:
                messagebox.showerror("Open image", f"Could not open:\n{e}")
                return
            # Stop camera to avoid races overwriting the chosen file image
            self.stop_camera()
            self._img_path = os.path.abspath(path)
            self._img_pil = img
            self._redraw()
            # tell the app we have a new image file path
            if callable(self._on_image_change):
                try:
                    self._on_image_change(self._img_path)
                except Exception:
                    pass

        def _redraw(self):
            if not hasattr(self, "_img_pil"):
                self.canvas.delete("all")
                return
            cw, ch = max(1, self.canvas.winfo_width()), max(1, self.canvas.winfo_height())
            img = self._img_pil
            # fit
            scale = min((cw - 2) / img.width, (ch - 2) / img.height, 1.0)
            disp = img if scale >= 0.999 else img.resize((int(img.width * scale), int(img.height * scale)),
                                                         Image.LANCZOS)
            self._img_tk = ImageTk.PhotoImage(disp)
            self.canvas.delete("all")
            self.canvas.create_image(cw // 2, ch // 2, image=self._img_tk)

        # ---- Camera ----
        def start_camera(self):
            if cv2 is None:
                print("[camera] OpenCV not installed. pip install opencv-python")
                messagebox.showinfo("Camera", "OpenCV not installed.\n\nRun: pip install opencv-python")
                return

            try:
                self._cam = cv2.VideoCapture(0, cv2.CAP_DSHOW) if os.name == "nt" else cv2.VideoCapture(0)
                if not self._cam or not self._cam.isOpened():
                    raise RuntimeError("No camera found")
                # entering live mode: new frames will update _img_pil; don't keep any stale _img_path
                self._live_mode = True
                self._img_path = None
                self._update_cam()
            except Exception as e:
                print(f"[camera] {e}")
                messagebox.showerror("Camera Error", f"Could not start camera:\n{e}")
                self._cam = None
                self._live_mode = False


        def _update_cam(self):
            # Keep pushing frames to _img_pil; DO NOT touch _img_path here (prevents race)
            if self._cam is None or not self._cam.isOpened():
                return
            ok, frame = self._cam.read()
            if ok:
                rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                self._img_pil = Image.fromarray(rgb)
                self._redraw()
            self._cam_timer = self.after(33, self._update_cam)

        def stop_camera(self):
            self._live_mode = False
            if self._cam_timer:
                try:
                    self.after_cancel(self._cam_timer)
                except Exception:
                    pass
                self._cam_timer = None
            if self._cam is not None:
                try:
                    self._cam.release()
                except Exception:
                    pass
                self._cam = None

        def snapshot(self):
            """
            Save current image/camera frame to ./out/snapshot_*.png.
            Returns the saved absolute path (string) on success, or None on failure.
            """
            if not hasattr(self, "_img_pil"):
                messagebox.showinfo("Snapshot", "No image/camera frame to save.")
                return None
            os.makedirs("out", exist_ok=True)
            ts = time.strftime("%Y%m%d_%H%M%S")
            path = os.path.abspath(os.path.join("out", f"snapshot_{ts}.png"))
            try:
                self._img_pil.save(path)
                self._img_path = path
                # notify app that current image changed to this snapshot
                if callable(self._on_image_change):
                    try:
                        self._on_image_change(self._img_path)
                    except Exception:
                        pass
                return path
            except Exception as e:
                print(f"[snapshot] {e}")
                return None

        # ---- Send to model ----
        def send_now(self):
            if not hasattr(self, "_img_pil"):
                print("[vision] No image/camera frame yet.")
                return

            # Ensure a file path; if the image is transient (camera frame), write a temp PNG in ./out
            path = self._img_path
            if path is None:
                os.makedirs("out", exist_ok=True)
                path = os.path.abspath(os.path.join("out", "live_frame.png"))
                try:
                    self._img_pil.save(path)
                    self._img_path = path
                    # also tell the app that the current image became this file
                    if callable(self._on_image_change):
                        try:
                            self._on_image_change(self._img_path)
                        except Exception:
                            pass
                except Exception as e:
                    print(f"[vision] could not save live frame: {e}")
                    return

            prompt = self.prompt_var.get().strip() or "Describe and solve any equations in this image. Use LaTeX."
            self._on_send(path, prompt)

        def destroy(self):
            self.stop_camera()
            super().destroy()

    def toggle_latex(self):
        try:
            if self.latex_win.state() == "withdrawn":
                self.latex_win.show()
            else:
                self.latex_win.hide()
        except Exception:
            self.latex_win.show()

    def open_avatar(self):
        try:
            kind = self.avatar_kind.get()

            # Close any existing avatar first
            if self.avatar_win and self.avatar_win.winfo_exists():
                try:
                    self.avatar_win.destroy()
                except:
                    pass
                self.avatar_win = None

            if kind == "Rings":
                self.avatar_win = CircleAvatarWindow(self.master)
            elif kind == "Rectangles":
                self.avatar_win = RectAvatarWindow(self.master)
            elif kind == "Rectangles 2":
                self.avatar_win = RectAvatarWindow2(self.master)
            elif kind == "Radial Pulse":
                self.avatar_win = RadialPulseAvatar(self.master)
            elif kind == "FaceRadialAvatar":
                self.avatar_win = FaceRadialAvatar(self.master)
            elif kind == "String Grid":  # ‚Üê ADDED STRING GRID HERE
                self.avatar_win = StringGridAvatar(self.master)
            elif kind == "Sphere":  # ‚Üê ADDED Sphere
                self.avatar_win = TextureMappedSphere(self.master)

            else:
                self.avatar_win = CircleAvatarWindow(self.master)

            if self.avatar_win:
                self.avatar_win.show()

        except Exception as e:
            self.logln(f"[avatar] Error opening avatar: {e}")


    def close_avatar(self):
        try:
            if self.avatar_win and self.avatar_win.winfo_exists():
                self.avatar_win.hide()
        except Exception as e:
            self.logln(f"[avatar] close error: {e}")

    def toggle_avatar(self):
        try:
            if self.avatar_win is None or not self.avatar_win.winfo_exists() or self.avatar_win.state() == "withdrawn":
                self.open_avatar()
            else:
                self.close_avatar()
        except Exception as e:
            self.logln(f"[avatar] toggle error: {e}")
            # Try to create a new one if there's an issue
            try:
                self.avatar_win = None
                self.open_avatar()
            except Exception as e2:
                self.logln(f"[avatar] recovery failed: {e2}")

    def toggle_search_window(self, ensure_visible=False):
        """Toggle web search window - FIXED VERSION
        Args:
            ensure_visible: If True, always show the window (used for voice searches)
        """
        try:
            if self.search_win is None or not self.search_win.winfo_exists():
                self.search_win = WebSearchWindow(self.master, log_fn=self.logln)
                # Bind ALL the search methods to this app instance
                self.search_win.brave_search = self.brave_search
                self.search_win.polite_fetch = self.polite_fetch
                self.search_win.guess_pubdate = self.guess_pubdate
                self.search_win.extract_images = self.extract_images
                self.search_win.extract_readable = self.extract_readable
                self.search_win.summarise_with_qwen = self.summarise_with_qwen
                self.search_win.synthesize_search_results = self.synthesize_search_results
                self.search_win.normalize_query = self.normalize_query
                self.search_win.play_search_results = self.play_search_results
                # ===  CRITICAL CONNECTIONS ===
                self.search_win.main_app = self  # Give access to main app
                self.search_win.preview_latex = self.preview_latex  # Use main app's method
                self.search_win.ensure_latex_window = self.ensure_latex_window  # Use main app's method
                self.search_win.logln = self.logln  # Use main app's logging



            # Always show the window if ensure_visible is True (for voice searches)
            # or if we're toggling and it's currently withdrawn
            if ensure_visible or self.search_win.state() == "withdrawn":
                self.search_win.deiconify()
                self.search_win.lift()
                self.search_win.focus_set()
            else:
                # Only hide if not forced to be visible
                if not ensure_visible:
                    self.search_win.withdraw()

        except Exception as e:
            self.logln(f"[search] window error: {e}")

    def _avatar_set_level_async(self, lvl: int):
        try:
            if self.avatar_win and self.avatar_win.winfo_exists():
                self.avatar_win.set_level(lvl)
        except Exception:
            pass

    def send_text(self):
        if hasattr(self, "text_box"):
            text = self.text_box.get("1.0", "end-1c").strip()
            self.text_box.delete("1.0", "end")
        else:
            text = self.text_entry.get().strip()
            self.text_entry.delete(0, "end")

        if not text:
            return

        threading.Thread(target=self.handle_text_query, args=(text,), daemon=True).start()

    def preview_latex(self, content: str, context="text"):
        """Preview LaTeX content with append/replace option - ENHANCED FOR SEARCH"""
        if not self.latex_auto.get():
            return

        def _go():
            try:
                latex_win = self.ensure_latex_window(context)
                latex_win.show()

                # === CHECK APPEND MODE ===
                if self.latex_append_mode.get():
                    # APPEND MODE - add to existing content
                    latex_win.append_document(content)
                    self.logln(f"[latex] üìù Appended to {context} window")
                else:
                    # REPLACE MODE - clear and show new content (original behavior)
                    latex_win.show_document(content)
                    self.logln(f"[latex] üîÑ Showing in {context} window (replace mode)")

                self._current_latex_context = context

            except Exception as e:
                self.logln(f"[latex] preview error ({context}): {e}")

        self.master.after(0, _go)

    def preview_search_results(self, content: str):
        """Special method for search results that preserves the display"""
        self.preview_latex(content, context="search")

    def _latex_theme(self, mode: str):
        try:
            # Set theme on the appropriate window
            if mode == "vision" and hasattr(self, "latex_win_vision"):
                self.latex_win_vision.set_scheme("vision")
            elif hasattr(self, "latex_win_text"):
                self.latex_win_text.set_scheme("default")
        except Exception:
            pass

    def synthesize_to_wav(self, text, out_wav, role="text"):
        """Synthesize text to WAV with enhanced math speaking"""

        # Check mute states first
        if role == "text" and self.text_ai_muted:
            self.logln("[mute] üîá Text AI muted - skipping TTS synthesis completely")
            return False
        elif role == "vision" and self.vision_ai_muted:
            self.logln("[mute] üîá Vision AI muted - skipping TTS synthesis completely")
            return False

        # Use the toggle setting
        speak_math = self.speak_math_var.get()
        clean_text = clean_for_tts(text, speak_math=speak_math)

        import time
        import tempfile
        engine = self.tts_engine.get()

        # FIXED: Enhanced file locking protection with proper temp file handling
        out_dir = os.path.dirname(out_wav) or "out"
        os.makedirs(out_dir, exist_ok=True)

        # Use tempfile in the same directory to avoid cross-device issues
        temp_fd, temp_path = tempfile.mkstemp(suffix='.wav', prefix='tts_', dir=out_dir)
        os.close(temp_fd)  # Close the file descriptor, we'll use the path

        max_retries = 3
        retry_delay = 0.5

        for attempt in range(max_retries):
            try:
                # Clean up any existing temp files
                if os.path.exists(temp_path):
                    try:
                        os.remove(temp_path)
                    except Exception as e:
                        self.logln(f"[tts] Warning: Could not remove temp file {temp_path}: {e}")
                        # Continue anyway, tempfile should be unique

                # Vision always uses the FIRST SAPI5 voice
                if role == "vision":
                    import pyttsx3
                    voice_id = self._sapi_default_voice_id
                    if not voice_id:
                        voice_id = self.sapi_voice_var.get().split(" | ")[0]
                    eng = pyttsx3.init()
                    eng.setProperty("voice", voice_id)
                    # === Speech TTS Speed ===
                    eng.setProperty("rate", 150 + self.speech_rate_var.get() * 10)

                    eng.save_to_file(clean_text, temp_path)
                    eng.runAndWait()
                    eng.stop()

                    # Wait for file to be written with timeout
                    wait_time = 0
                    while not os.path.exists(temp_path) and wait_time < 5.0:
                        time.sleep(0.1)
                        wait_time += 0.1

                    if os.path.exists(temp_path) and os.path.getsize(temp_path) > 1024:  # At least 1KB
                        # Safely replace the output file
                        if os.path.exists(out_wav):
                            try:
                                os.remove(out_wav)
                            except Exception as e:
                                self.logln(f"[tts] Warning: Could not remove existing output {out_wav}: {e}")
                                # Try to continue anyway

                        try:
                            os.rename(temp_path, out_wav)
                            self.logln(f"[tts] vision (sapi5, fixed): {voice_id}")
                            return True
                        except Exception as e:
                            self.logln(f"[tts] Error renaming temp file: {e}")
                            # Fallback: copy instead of rename
                            try:
                                import shutil
                                shutil.copy2(temp_path, out_wav)
                                self.logln(f"[tts] vision (sapi5, fixed) - copied: {voice_id}")
                                return True
                            except Exception as e2:
                                self.logln(f"[tts] Error copying temp file: {e2}")
                    else:
                        self.logln(
                            f"[tts] temp file not created properly: exists={os.path.exists(temp_path)}, size={os.path.getsize(temp_path) if os.path.exists(temp_path) else 0}")
                        continue

                # Text keeps current selection
                if engine == "sapi5":
                    import pyttsx3
                    # Get the selected display name
                    selected_display = self.sapi_voice_var.get()

                    # Use voice mapping for Text AI if available, otherwise fallback
                    if hasattr(self, 'voice_mapping') and selected_display in self.voice_mapping:
                        voice_id = self.voice_mapping[selected_display]
                    else:
                        voice_id = selected_display.split(" | ")[0] if " | " in selected_display else selected_display

                    eng = pyttsx3.init()
                    eng.setProperty("voice", voice_id)
                    # === TTS Speed  ===
                    eng.setProperty("rate", 150 + self.speech_rate_var.get() * 10)

                    eng.save_to_file(clean_text, temp_path)
                    eng.runAndWait()
                    eng.stop()

                    # Wait for file to be written with timeout
                    wait_time = 0
                    while not os.path.exists(temp_path) and wait_time < 5.0:
                        time.sleep(0.1)
                        wait_time += 0.1

                    if os.path.exists(temp_path) and os.path.getsize(temp_path) > 1024:  # At least 1KB
                        # Safely replace the output file
                        if os.path.exists(out_wav):
                            try:
                                os.remove(out_wav)
                            except Exception as e:
                                self.logln(f"[tts] Warning: Could not remove existing output {out_wav}: {e}")
                                # Try to continue anyway

                        try:
                            os.rename(temp_path, out_wav)
                            self.logln(f"[tts] text (sapi5): {voice_id}")
                            return True
                        except Exception as e:
                            self.logln(f"[tts] Error renaming temp file: {e}")
                            # Fallback: copy instead of rename
                            try:
                                import shutil
                                shutil.copy2(temp_path, out_wav)
                                self.logln(f"[tts] text (sapi5) - copied: {voice_id}")
                                return True
                            except Exception as e2:
                                self.logln(f"[tts] Error copying temp file: {e2}")
                    else:
                        self.logln(
                            f"[tts] temp file not created properly: exists={os.path.exists(temp_path)}, size={os.path.getsize(temp_path) if os.path.exists(temp_path) else 0}")
                        continue

            except Exception as e:
                self.logln(f"[tts] attempt {attempt + 1} failed: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                else:
                    self.logln(f"[tts] all {max_retries} attempts failed")
                    # Final cleanup
                    try:
                        if os.path.exists(temp_path):
                            os.remove(temp_path)
                    except Exception:
                        pass
                    return False

        # Final cleanup if we exit the loop without success
        try:
            if os.path.exists(temp_path):
                os.remove(temp_path)
        except Exception:
            pass

        return False

    def set_speech_rate(self, rate: int):
        """Set speech rate to specific value"""
        self.speech_rate_var.set(rate)
        self.update_rate_display()
        self.logln(f"[tts] Speech rate set to: {rate}")

    def update_rate_display(self):
        """Update the rate display label"""
        rate = self.speech_rate_var.get()
        if hasattr(self, 'rate_value_label'):
            if rate < -5:
                self.rate_value_label.config(text="Very Slow")
            elif rate < 0:
                self.rate_value_label.config(text="Slow")
            elif rate == 0:
                self.rate_value_label.config(text="Normal")
            elif rate <= 5:
                self.rate_value_label.config(text="Fast")
            else:
                self.rate_value_label.config(text="Very Fast")

    # === Device Methods ===
    def _device_hostapi_name(self, index):
        try:
            if index is None:
                return None
            info = sd.query_devices(index)
            hostapi_idx = info.get('hostapi', None)
            if hostapi_idx is None:
                return None
            hai = sd.query_hostapis(hostapi_idx)
            return hai.get('name')
        except Exception:
            return None

    def _list_output_devices(self):
        try:
            info = sd.query_devices()
            outs = []
            for i, d in enumerate(info):
                if d.get('max_output_channels', 0) > 0:
                    name = d.get('name', f'Device {i}')
                    outs.append(f"{i}: {name}")
            return outs if outs else ["(default output)"]
        except Exception as e:
            self.logln(f"[audio] output device query failed: {e}")
            return ["(default output)"]

    def _selected_out_device_index(self):
        try:
            choice = self.out_combo.get()
            return int(choice.split(":")[0]) if ":" in choice else None
        except Exception:
            return None

    # === Vision UI Helpers ===
    def _ensure_image_window(self):
        """Create the ImageWindow if needed (but don't show it)."""
        try:
            if not hasattr(self, "_img_win") or self._img_win is None or not self._img_win.winfo_exists():
                self._img_win = self.ImageWindow(
                    self.master,
                    on_send=self.ask_vision,
                    on_image_change=self._on_new_image
                )
        except Exception as e:
            self.logln(f"[vision] could not create image window: {e}")
            self._img_win = None

    def start_camera_ui(self):
        """Voice/typed: 'start camera' -> open window and start streaming."""
        try:
            self._ensure_image_window()
            if self._img_win is None:
                self.logln("[vision] camera UI unavailable")
                return
            self._img_win.deiconify()
            self._img_win.lift()
            self._img_win.start_camera()
            self.logln("[vision] camera started")
        except Exception as e:
            self.logln(f"[vision] start camera error: {e}")

    def stop_camera_ui(self):
        """Voice/typed: 'stop camera' -> stop streaming."""
        try:
            if hasattr(self, "_img_win") and self._img_win and self._img_win.winfo_exists():
                self._img_win.stop_camera()
                self.logln("[vision] camera stopped")
            else:
                self.logln("[vision] camera window not open")
        except Exception as e:
            self.logln(f"[vision] stop camera error: {e}")

    def take_picture_ui(self):
        """Voice/typed: 'take a picture' -> snapshot current frame."""
        try:
            self._ensure_image_window()
            if self._img_win is None:
                self.logln("[vision] camera UI unavailable")
                return
            saved = self._img_win.snapshot()
            if saved:
                self.logln(f"[vision] snapshot ready: {saved}")
            else:
                self.logln("[vision] snapshot failed")
        except Exception as e:
            self.logln(f"[vision] take picture error: {e}")

    def explain_last_image_ui(self, prompt_text=None):
        try:
            img_path = None
            if hasattr(self, "_img_win") and self._img_win and self._img_win.winfo_exists():
                img_path = getattr(self._img_win, "_img_path", None)
            if not img_path:
                img_path = self._last_image_path

            if not img_path:
                self.logln("[vision] no image available. Say 'take a picture' or open an image first.")
                return

            prompt = (prompt_text or "Describe what is in the image in clear detail.").strip()
            self.ask_vision(img_path, prompt)
        except Exception as e:
            self.logln(f"[vision] explain image error: {e}")

    def _ollama_available_models(self):
        try:
            r = requests.get("http://127.0.0.1:11434/api/tags", timeout=5)
            r.raise_for_status()
            tags = r.json().get("models", [])
            return {m.get("name", "") for m in tags}
        except Exception as e:
            self.logln(f"[ollama] tag query failed: {e}")
            return set()

    def _ensure_vl_model_installed(self):
        have = self._ollama_available_models()
        want = str(self.vl_model or "").strip()
        if not want:
            self.vl_model = "qwen2.5vl:7b"
            want = self.vl_model
        if want in have:
            self.logln(f"[vision] ‚úÖ using VL model: {want}")
            return
        candidates = ["qwen2.5vl:7b", "qwen2.5vl:latest", "qwen2.5vl-latex:latest"]
        for c in candidates:
            if c in have:
                self.logln(f"[vision] '{want}' not found; switching to '{c}'")
                self.vl_model = c
                return
        self.logln(f"[vision] ‚ö†Ô∏è '{want}' not installed. Run:  ollama pull {want}")

    def _load_personalities(self):
        """Load personality profiles from the Personalities folder"""
        personalities_dir = "Personalities"
        self.personalities = {
            "Default": {
                "name": "Default",
                "description": "Standard assistant behavior",
                "is_personality": True
            }
        }

        try:
            # Get the current working directory
            current_dir = os.getcwd()
            self.logln(f"[personality] Current directory: {current_dir}")

            # Check if Personalities directory exists
            personalities_path = os.path.join(current_dir, personalities_dir)
            self.logln(f"[personality] Looking for: {personalities_path}")

            if not os.path.exists(personalities_path):
                os.makedirs(personalities_path)
                self.logln(f"[personality] ‚ùó Created directory: {personalities_path}")
                return

            # List all files in the directory
            all_files = os.listdir(personalities_path)
            self.logln(f"[personality] Files in directory: {all_files}")

            # Load all JSON files from the directory
            personality_files = [f for f in all_files if f.lower().endswith('.json')]
            self.logln(f"[personality] JSON files found: {personality_files}")

            if not personality_files:
                self.logln("[personality] No personality files found, using Default only")
                return

            for filename in personality_files:
                filepath = os.path.join(personalities_path, filename)
                self.logln(f"[personality] Trying to load: {filepath}")

                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        personality_data = json.load(f)

                    # Use the name from the file, or filename as fallback
                    personality_name = personality_data.get('name', os.path.splitext(filename)[0])
                    self.personalities[personality_name] = personality_data
                    self.logln(f"[personality] ‚úÖ SUCCESS: Loaded {personality_name}")

                except Exception as e:
                    self.logln(f"[personality] ‚ùå ERROR loading {filename}: {e}")

            # Update the combo box values
            personality_names = list(self.personalities.keys())
            self.logln(f"[personality] Final personalities: {personality_names}")
            self.personality_combo['values'] = personality_names

            self.logln(f"[personality] Loaded {len(self.personalities)} personalities")

        except Exception as e:
            self.logln(f"[personality] ‚ùå CRITICAL ERROR: {e}")

            # Finished Update personalities

    def _on_personality_change(self, event=None):
        """Called when user selects a new personality"""
        selected_name = self.personality_var.get()
        if selected_name and selected_name in self.personalities:
            self.apply_personality(selected_name)

    def _enhance_system_prompt(self, personality_prompt):
        """Combine personality prompt with original system prompt"""

        # Get the ORIGINAL system prompt from your config
        original_prompt = self.cfg.get("system_prompt") or self.cfg.get("qwen_system_prompt", "")

        # If we have both, combine them intelligently
        if original_prompt and personality_prompt:
            combined_prompt = f"""{personality_prompt}

    ADDITIONAL CONTEXT:
    {original_prompt}
    """
            return combined_prompt

        # If only personality prompt, use it with base instructions
        elif personality_prompt:
            return personality_prompt

        # Fallback
        return original_prompt or ""

    def apply_personality(self, personality_name):
        """Apply personality settings - PRESERVE original settings if not specified"""
        if personality_name not in self.personalities:
            self.logln(f"[personality] ‚ùå Personality '{personality_name}' not found")
            return

        personality = self.personalities[personality_name]

        try:
            # For "Default" personality, reset to main config
            if personality_name == "Default":
                self._reset_to_default_personality()
                self.personality_status.config(text="‚úì Default", foreground="green")
                self.logln("[personality] ‚úÖ Restored default settings")
                return

            self.logln(f"[personality] Applying {personality_name}...")

            # 1. Apply voice settings - ONLY if specified in personality
            voice_settings = personality.get('voice', {})

            # SAPI Voice selection - ONLY change if personality specifies it
            target_voice = voice_settings.get('sapi_voice')
            if target_voice:
                # Find the best matching voice in available voices
                available_voices = list(self.voice_mapping.keys())
                matching_voices = [v for v in available_voices
                                   if target_voice.lower() in v.lower()]

                if matching_voices:
                    self.sapi_voice_var.set(matching_voices[0])
                    self.logln(f"[personality] Voice set to: {matching_voices[0]}")
                else:
                    self.logln(f"[personality] Voice '{target_voice}' not found, keeping current")
            else:
                self.logln(f"[personality] No voice specified, keeping current")

            # Speech rate - ONLY change if specified
            speech_rate = voice_settings.get('speech_rate')
            if speech_rate is not None:
                self.speech_rate_var.set(speech_rate)
                self.update_rate_display()
                self.logln(f"[personality] Speech rate: {speech_rate}")
            else:
                self.logln(f"[personality] No speech rate specified, keeping current")

            # 2. Apply system prompt - CRITICAL FIX: ADD DATE/TIME CONTEXT
            system_prompt = personality.get('system_prompt')
            if system_prompt:
                # Get current date/time
                from datetime import datetime
                current_datetime = datetime.now()
                current_date = current_datetime.strftime("%B %d, %Y")
                current_time = current_datetime.strftime("%I:%M %p")
                current_day = current_datetime.strftime("%A")

                # Create the enhanced prompt WITH date/time
                enhanced_prompt = f"""CURRENT REAL DATE: {current_day}, {current_date} at {current_time}
        *** USE THIS EXACT DATE - DO NOT CALCULATE OR GUESS ***

        CRITICAL RULES:
        1. When asked about the date, ALWAYS use: "{current_day}, {current_date}"
        2. Never calculate days of the week from dates yourself
        3. If your internal knowledge conflicts with the date above, TRUST THE DATE PROVIDED
        4. October 22, 2025 is {current_day} - your training data may be incorrect for this specific date

        USE THIS INFORMATION WHEN:
        - Specifically asked about date, time, or scheduling
        - Questions require current time context
        - Making time-sensitive calculations

        DO NOT:
        - Calculate or guess days of the week
        - Use your internal calendar knowledge for date questions
        - Second-guess the provided current date

        For casual conversation, respond naturally without unnecessary date/time mentions.

        ===== PERSONALITY PROMPT =====
        {system_prompt}
        """

                self.qwen.system_prompt = enhanced_prompt
                self.logln(f"[personality] ‚úÖ System prompt updated with date/time awareness")
                self.logln(f"[personality] üìÖ Current date in prompt: {current_date} at {current_time}")
            else:
                # If no system prompt in personality, use the enhanced default
                self._reset_to_default_personality()
                self.logln(f"[personality] No system prompt specified, using enhanced default")

            # Update status
            self.personality_status.config(text=f"‚úì {personality_name}", foreground="blue")

            self.logln(f"[personality] ‚úÖ Successfully applied: {personality_name}")

            # Play confirmation sound
            self.play_chime(freq=660, ms=200, vol=0.15)

        except Exception as e:
            self.logln(f"[personality] ‚ùå Error applying {personality_name}: {e}")
            self.personality_status.config(text="‚ùå Error", foreground="red")



    def _reset_to_default_personality(self):
        """Reset to settings from main config.json"""
        # Reset voice to config default
        config_voice = self.cfg.get("text_ai_voice")
        if config_voice:
            available_voices = list(self.voice_mapping.keys())
            matching_voices = [v for v in available_voices if config_voice in v]
            if matching_voices:
                self.sapi_voice_var.set(matching_voices[0])

        # Reset speech rate
        default_rate = self.cfg.get("text_ai_speech_rate", 0)
        self.speech_rate_var.set(default_rate)
        self.update_rate_display()

        # Reset system prompt to original
        original_prompt = self.cfg.get("system_prompt") or self.cfg.get("qwen_system_prompt", "")
        if original_prompt:
            self.qwen.system_prompt = original_prompt

    # === SEARCH METHODS ===

    def brave_search(self, query: str, count: int = 6):
        brave_key = os.getenv("BRAVE_KEY")
        if not brave_key:
            raise RuntimeError("No BRAVE_KEY found in environment")
        # === Logs we are searching the Internet ===
        self.logln(f"[SEARCH] üöÄ Calling Brave API: '{query}'")

        endpoint = "https://api.search.brave.com/res/v1/web/search"
        headers = {"X-Subscription-Token": brave_key, "User-Agent": "LocalAI-ResearchBot/1.0"}
        params = {"q": query, "count": count}

        with httpx.Client(timeout=25.0, headers=headers) as client:
            r = client.get(endpoint, params=params)
            r.raise_for_status()
            data = r.json()

        out = []
        for w in (data.get("web", {}) or {}).get("results", []):
            out.append(
                Item(title=w.get("title", "No title"), url=w.get("url", ""), snippet=w.get("description", "")))
            # === check what its searching  ===
            self.logln(f"[BRAVE API] ‚úÖ Found {len(out)} results for '{query}'")

        return out

    def polite_fetch(self, url: str):
        headers = {"User-Agent": "LocalAI-ResearchBot/1.0"}
        try:
            with httpx.Client(timeout=25.0, headers=headers, follow_redirects=True) as client:
                r = client.get(url)
                r.raise_for_status()
                return r.text
        except Exception:
            return None

    def extract_readable(self, html: str, url: str = None):
        text = trafilatura.extract(html, url=url, include_links=False, include_formatting=False)
        return text or ""

    def guess_pubdate(self, html: str):
        try:
            soup = BeautifulSoup(html, "lxml")
        except Exception:
            return None

        metas = [
            ("property", "article:published_time"), ("property", "og:published_time"),
            ("property", "og:updated_time"), ("name", "pubdate"), ("name", "publication_date"),
            ("name", "date"), ("name", "dc.date"), ("name", "dc.date.issued"),
            ("name", "sailthru.date"), ("itemprop", "datePublished"), ("itemprop", "dateModified"),
        ]

        for key, val in metas:
            tag = soup.find("meta", attrs={key: val})
            if tag and tag.get("content"):
                return tag["content"]

        t = soup.find("time")
        if t and (t.get("datetime") or (t.text and t.text.strip())):
            return t.get("datetime") or t.text.strip()
        return None

    def summarise_for_ai_search(self, text: str, url: str, pubdate: str):
        """Enhanced summarization that preserves practical information"""
        text = text[:18000]

        # Enhanced date context
        if pubdate:
            date_context = f"PUBLICATION DATE: {pubdate}\n"
        else:
            import re
            date_matches = re.findall(
                r'\b(?:20\d{2}[-/]\d{1,2}[-/]\d{1,2}|\d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* 20\d{2})\b',
                text[:3000])
            date_context = f"MENTIONED DATES: {', '.join(date_matches[:3])}\n" if date_matches else ""

        # DETECT QUERY TYPE AND ADAPT SUMMARIZATION
        query_lower = getattr(self, '_last_search_query', '').lower()

        # Flight/travel related queries
        if any(keyword in query_lower for keyword in ['flight', 'fly', 'airline', 'airport', 'travel to']):
            summary_prompt = (
                "Extract COMPLETE flight information with these details:\n\n"
                "## FLIGHT INFORMATION\n"
                "- Airline names and flight numbers\n"
                "- Departure and arrival airports (with codes if available)\n"
                "- Departure and arrival times/dates\n"
                "- Flight duration\n"
                "- Prices and fare classes\n"
                "- Stopovers/layovers\n"
                "- Booking links or airline websites\n\n"
                "## TRAVEL DETAILS\n"
                "- Airport locations and terminals\n"
                "- Booking requirements\n"
                "- Baggage information\n"
                "- Recent deals or promotions\n\n"
                "Include ALL specific numbers, times, prices, and codes. Be very detailed about schedules and availability.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        # Business/location queries
        elif any(keyword in query_lower for keyword in
                 ['address', 'location', 'where is', 'hours', 'contact', 'phone', 'email']):
            summary_prompt = (
                "EXTRACT ONLY INFORMATION EXPLICITLY STATED IN THE TEXT. NEVER CREATE PLACEHOLDERS OR INVENT INFORMATION.\n\n"
                "CRITICAL RULES:\n"
                "1. ONLY include information that appears VERBATIM in the source text\n"
                "2. NEVER use brackets [ ], parentheses ( ), or placeholder text\n"
                "3. If a website is mentioned, copy the EXACT URL\n"
                "4. If information is missing, OMIT that line entirely\n"
                "5. Do NOT create template responses\n\n"
                "EXTRACTED INFORMATION (ONLY IF FOUND):\n"
                "- Business Name: [copy exact name if found]\n"
                "- Address: [copy exact address if found]\n"
                "- Phone: [copy exact phone number if found]\n"
                "- Email: [copy exact email if found]\n"
                "- Website: [copy exact URL if found]\n"
                "- Hours: [copy exact hours if found]\n\n"
                "EXAMPLES - WRONG:\n"
                "‚ùå Address: [Address may vary]\n"
                "‚ùå Phone: [Phone number may vary]  \n"
                "‚ùå Website: [Website Link]\n"
                "‚ùå Website: [Website URL if available]\n\n"
                "EXAMPLES - CORRECT:\n"
                "‚úÖ Address: 456 Northshore Road, Unit 2, Glenfield 0678\n"
                "‚úÖ Phone: +64 9 483 5555\n"
                "‚úÖ Website: https://www.serenityspa.co.nz\n"
                "‚úÖ Website: www.serenityspa.com\n"
                "‚úÖ (omit Website line if no URL found)\n\n"
                "If the text contains '456 Glenfield Road, Unit 2, Glenfield 0678' and '+64 9 483 5555' but NO website, output:\n"
                "Address: 456 Glenfield Road, Unit 2, Glenfield 0678\n"
                "Phone: +64 9 483 5555\n\n"
                "DO NOT INVENT WEBSITE INFORMATION. If no website is found, omit the Website line completely.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )


        # Product/service queries
        elif any(keyword in query_lower for keyword in ['price', 'cost', 'buy', 'purchase', 'deal', 'sale']):
            summary_prompt = (
                "Extract COMPLETE product/service information:\n\n"
                "## PRICING & AVAILABILITY\n"
                "- Exact prices and currency\n"
                "- Model numbers/specifications\n"
                "- Availability status\n"
                "- Seller/retailer information\n"
                "- Shipping costs and delivery times\n"
                "- Return policies\n\n"
                "## PRODUCT DETAILS\n"
                "- Features and specifications\n"
                "- Dimensions/sizes\n"
                "- Colors/options available\n"
                "- Warranty information\n\n"
                "Include ALL pricing, specifications, and purchase details. Be very specific about numbers and options.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        # Weather queries
        elif any(keyword in query_lower for keyword in
                 ['weather', 'forecast', 'temperature', 'rain', 'snow', 'humidity']):
            summary_prompt = (
                "Extract COMPLETE weather forecast information:\n\n"
                "## CURRENT CONDITIONS\n"
                "- Temperature and feels-like temperature\n"
                "- Weather description (sunny, rainy, etc.)\n"
                "- Humidity, wind speed and direction\n"
                "- Precipitation chances\n"
                "- Air quality and UV index\n\n"
                "## FORECAST\n"
                "- Hourly and daily forecasts\n"
                "- High/low temperatures\n"
                "- Severe weather alerts\n"
                "- Sunrise/sunset times\n\n"
                "## LOCATION DETAILS\n"
                "- Specific city/region\n"
                "- Geographic details if available\n"
                "- Timezone information\n\n"
                "Include ALL numerical weather data, times, and location specifics.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        else:
            # General comprehensive summary (for news, general info, etc.)
            summary_prompt = (
                "Create a COMPREHENSIVE summary that PRESERVES practical information:\n\n"
                "## ESSENTIAL DETAILS\n"
                "- Full names of businesses, people, organizations\n"
                "- Complete addresses, phone numbers, contact information\n"
                "- Prices, costs, financial figures\n"
                "- Dates, times, schedules\n"
                "- Locations, coordinates, directions\n"
                "- Website URLs, email addresses\n\n"
                "## KEY INFORMATION\n"
                "- Main facts and findings\n"
                "- Important numbers and statistics\n"
                "- Recent developments\n"
                "- Contact methods\n\n"
                "## ADDITIONAL CONTEXT\n"
                "- Background information\n"
                "- Related services or options\n"
                "- User reviews or ratings if available\n\n"
                "CRITICAL: NEVER omit addresses, phone numbers, prices, or contact information. Include them verbatim.\n"
                f"{date_context}"
                f"Source: {url}\n\nCONTENT TO SUMMARIZE:\n{text}"
            )

        try:
            payload = {
                "model": "qwen2.5:7b-instruct",
                "prompt": summary_prompt,
                "stream": False,
                "temperature": 0.1,  # Lower temperature for more factual accuracy
                "max_tokens": 1200  # More tokens for detailed information
            }

            with httpx.Client(timeout=75.0) as client:
                r = client.post("http://localhost:11434/api/generate", json=payload)
                r.raise_for_status()
                response = r.json().get("response", "").strip()

                # Enhanced fallback for better information extraction
                if len(response) < 100 or "no information" in response.lower():
                    return self._extract_practical_information(text[:12000], query_lower)

                return response

        except Exception as e:
            return self._extract_practical_information(text[:10000], query_lower)

    def _extract_practical_information(self, text: str, query_type: str) -> str:
        """Enhanced fallback extraction focusing on practical information"""
        import re

        sections = []

        # Enhanced address extraction
        address_patterns = [
            # Standard street addresses
            r'\b\d+\s+[A-Za-z0-9\s,]+(?:Street|St|Avenue|Ave|Road|Rd|Boulevard|Blvd|Lane|Ln|Drive|Dr|Court|Ct|Place|Pl|Way|Highway|Hwy)\.?\s*(?:#\s*\d+)?\s*,\s*[A-Za-z\s]+,\s*[A-Z]{2}\s*\d{5}(?:-\d{4})?\b',
            # Basic address format
            r'\b\d+\s+[\w\s]+\s+(?:St|Street|Ave|Avenue|Rd|Road|Blvd|Boulevard|Dr|Drive|Ln|Lane|Ct|Court),\s*[\w\s]+,\s*[A-Z]{2}\b',
            # PO Boxes
            r'\b(?:P\.?O\.?\s*Box|PO Box|P O Box)\s+\d+[^.!?]*',
        ]

        addresses = []
        for pattern in address_patterns:
            found = re.findall(pattern, text, re.IGNORECASE)
            addresses.extend(found)

        # Filter out obviously fake or placeholder addresses
        real_addresses = []
        for addr in addresses:
            addr_lower = addr.lower()
            # Skip placeholder text
            if any(placeholder in addr_lower for placeholder in
                   ['address may vary', 'varies', 'please contact', 'call for', 'not available']):
                continue
            # Skip if it's just a city/state without street
            if re.match(r'^[A-Za-z\s]+,?\s*[A-Z]{2}\s*\d{5}', addr) and not re.search(r'\d+', addr):
                continue
            real_addresses.append(addr.strip())

        if real_addresses:
            sections.append("## ADDRESSES FOUND")
            sections.extend([f"- {addr}" for addr in set(real_addresses)[:3]])
        # Extract website URLs (more comprehensive)
        url_patterns = [
            r'https?://[^\s<>"{}|\\^`\[\]]+',
            r'www\.[^\s<>"{}|\\^`\[\]]+\.[a-z]{2,}',
            r'[a-z0-9.-]+\.[a-z]{2,}/[^\s<>"{}|\\^`\[\]]*',
        ]

        urls = []
        for pattern in url_patterns:
            urls.extend(re.findall(pattern, text, re.IGNORECASE))

        # Filter and clean URLs
        clean_urls = []
        for url in urls:
            # Remove trailing punctuation
            url = re.sub(r'[.,;:!?)]+$', '', url)
            # Skip common false positives
            if any(bad in url.lower() for bad in ['example.com', 'website.com', 'yourwebsite', 'domain.com']):
                continue
            # Ensure it looks like a real URL
            if '.' in url and len(url) > 8:
                # Add http:// if missing for www URLs
                if url.startswith('www.') and not url.startswith('http'):
                    url = 'https://' + url
                clean_urls.append(url)

        if clean_urls:
            sections.append("\n## WEBSITES")
            sections.extend([f"- {url}" for url in set(clean_urls)[:3]])

        # Extract phone numbers
        phone_pattern = r'(\+?\d{1,2}?[-.\s]?\(?\d{3}\)?[-.\s]?\d{3}[-.\s]?\d{4})'
        phones = re.findall(phone_pattern, text)
        if phones:
            sections.append(f"\n## PHONE NUMBERS: {', '.join(set(phones)[:3])}")

        # Extract email addresses
        emails = re.findall(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text)
        if emails:
            sections.append(f"\n## EMAIL ADDRESSES: {', '.join(set(emails)[:3])}")

        # Extract prices and costs
        prices = re.findall(r'\$?\d+(?:,\d+)*(?:\.\d+)?\s*(?:dollars?|USD|‚Ç¨|¬£|¬•)?', text)
        if prices:
            sections.append(f"\n## PRICES MENTIONED: {', '.join(set(prices)[:8])}")

        # Flight-specific extraction
        if 'flight' in query_type:
            flight_info = re.findall(r'[A-Z]{2}\d+\s+.*?(?:\d{1,2}:\d{2}|AM|PM)', text)
            if flight_info:
                sections.append("\n## FLIGHT DETAILS")
                sections.extend([f"- {info}" for info in flight_info[:5]])

        # Business hours
        hours = re.findall(
            r'(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun)[a-z]*.*?\d{1,2}:\d{2}\s*(?:AM|PM)?.*?\d{1,2}:\d{2}\s*(?:AM|PM)?', text,
            re.IGNORECASE)
        if hours:
            sections.append("\n## BUSINESS HOURS")
            sections.extend([f"- {hour}" for hour in hours[:3]])

        # Weather data extraction
        if 'weather' in query_type:
            temps = re.findall(r'\b\d{1,3}¬∞?F?\b', text)
            if temps:
                sections.append(f"\n## TEMPERATURES: {', '.join(set(temps)[:6])}")

        # If we found practical information, return it
        if sections:
            return "\n".join(sections)
        else:
            # Return meaningful content lines as fallback
            lines = [line.strip() for line in text.split('\n') if len(line.strip()) > 30]
            return "## KEY INFORMATION EXTRACTED\n" + "\n".join([f"- {line}" for line in lines[:10]])

    def _extract_detailed_news(self, text: str) -> str:
        """Enhanced fallback extraction with more structure"""
        import re

        # Extract key information with more context
        sections = []

        # Headlines and key sentences
        sentences = re.split(r'[.!?]+', text)
        key_sentences = []

        important_indicators = [
            'announced', 'reported', 'confirmed', 'revealed', 'disclosed',
            'investigation', 'charged', 'arrested', 'settlement', 'agreement',
            'election', 'resigned', 'appointed', 'launched', 'released',
            'fire', 'accident', 'killed', 'injured', 'missing', 'found',
            'storm', 'flood', 'earthquake', 'weather', 'forecast', 'temperature',
            'budget', 'funding', 'cost', 'price', 'investment'
        ]

        for sentence in sentences:
            sentence = sentence.strip()
            if (len(sentence) > 25 and
                    any(indicator in sentence.lower() for indicator in important_indicators)):
                key_sentences.append(sentence)
                if len(key_sentences) >= 12:
                    break

        if key_sentences:
            sections.append("## KEY DEVELOPMENTS")
            sections.extend([f"- {s}" for s in key_sentences[:10]])

        # Extract numbers and statistics
        numbers = re.findall(r'\b(\$?[¬£‚Ç¨]?\d+(?:,\d+)*(?:\.\d+)?[%‚Ç¨¬£$]?(?:\s*(?:million|billion|thousand))?)\b',
                             text[:5000])
        if numbers:
            sections.append(f"\n## KEY NUMBERS: {', '.join(set(numbers[:8]))}")

        # Extract locations
        locations = re.findall(r'\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\b', text[:3000])
        unique_locs = list(
            set([loc for loc in locations if len(loc) > 3 and loc not in ['The', 'This', 'That', 'There', 'Here']]))
        if unique_locs:
            sections.append(f"\n## MENTIONED LOCATIONS: {', '.join(unique_locs[:6])}")

        if sections:
            return "\n".join(sections)
        else:
            # Last resort: return structured excerpt
            lines = text.split('\n')
            meaningful_lines = [line.strip() for line in lines if len(line.strip()) > 40][:8]
            return "## CONTENT OVERVIEW\n" + "\n".join([f"- {line}" for line in meaningful_lines])

    def summarise_with_qwen(self, text: str, url: str, pubdate: str):
        text = text[:20000]  # Limit text length
        pd_line = f"(Publish/Update date: {pubdate})\n" if pubdate else ""

        # FIRST PASS: Extract mathematical content specifically
        math_prompt = (
            "Extract ALL mathematical equations, formulas, and technical content from the following text. "
            "Preserve them exactly in their original LaTeX format ($$...$$, \\[...\\], $...$, etc.).\n"
            "Include:\n"
            "- All equations and formulas\n"
            "- Mathematical expressions\n"
            "- Chemical formulas\n"
            "- Code snippets\n"
            "- Important technical definitions\n"
            "Output the mathematical/technical content exactly as found, without summarization.\n"
            f"{pd_line}"
            f"Source: {url}\n\nCONTENT:\n{text[:10000]}"  # Use first 10k chars for math extraction
        )

        # SECOND PASS: Create a summary that REFERENCES the preserved math
        summary_prompt = (
            "Create a comprehensive summary (10-15 bullet points) that includes:\n"
            "- Key findings and conclusions\n"
            "- Important data points and results\n"
            "- References to mathematical content (say 'see equation X' or 'the formula shows')\n"
            "- Main arguments and evidence\n"
            "- Do NOT remove technical details - include them in context\n"
            "- Preserve specific numbers, measurements, and quantitative results\n"
            "Be detailed enough to be useful for technical analysis.\n"
            f"{pd_line}"
            f"Source: {url}\n\nCONTENT:\n{text}"
        )

        try:
            # Get mathematical content
            math_content = self.qwen.generate(math_prompt)

            # Get comprehensive summary
            summary = self.qwen.generate(summary_prompt)

            # Combine both with clear separation
            combined_result = f"MATHEMATICAL CONTENT:\n{math_content}\n\nSUMMARY:\n{summary}"

            return combined_result

        except Exception:
            # Fallback: Use a more math-friendly single prompt
            fallback_prompt = (
                "Create a DETAILED technical summary (12-18 bullet points) that PRESERVES all mathematical content.\n"
                "CRITICAL: Keep ALL equations, formulas, and LaTeX expressions exactly as they appear.\n"
                "Include:\n"
                "- Complete equations in $$...$$, \\[...\\], $...$ format\n"
                "- Mathematical proofs and derivations\n"
                "- Chemical formulas and reactions\n"
                "- Code snippets and algorithms\n"
                "- Quantitative results with exact numbers\n"
                "- Do NOT simplify or remove technical details\n"
                "- Focus on preserving the mathematical richness of the content\n"
                f"{pd_line}"
                f"Source: {url}\n\nCONTENT:\n{text}"
            )
            try:
                payload = {"model": "qwen2.5:7b-instruct", "prompt": fallback_prompt, "stream": False}
                with httpx.Client(timeout=90.0) as client:
                    r = client.post("http://localhost:11434/api/generate", json=payload)
                    r.raise_for_status()
                    return r.json().get("response", "").strip()
            except Exception as e:
                return f"Summarization failed: {e}"

    def extract_images(self, html: str, base_url: str, limit: int = 3):
        urls = []
        try:
            soup = BeautifulSoup(html, "lxml")
        except Exception:
            return urls

        for img in soup.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:") or re.search(r"\.svg($|\?)", src, re.I):
                continue

            alt = (img.get("alt") or "").lower()
            src_l = src.lower()
            if any(k in src_l for k in ["sprite", "icon", "logo", "ads", "advert", "pixel"]):
                continue
            if any(k in alt for k in ["icon", "logo"]):
                continue

            full = urljoin(base_url, src)
            if full not in urls:
                urls.append(full)
            if len(urls) >= limit:
                break
        return urls

    def synthesize_search_results(self, text: str):
        """Speak search results using DEDICATED search window"""

        # === STOP PROGRESS INDICATOR IMMEDIATELY ===
        self.stop_search_progress_indicator()

        def _tts_worker():
            if not text or not text.strip():
                return

            try:
                # Use math speaking for search results too
                speak_math = getattr(self, 'speak_math_var', tk.BooleanVar(value=True)).get()
                clean_tts_text = clean_for_tts(text, speak_math=speak_math)

                # === CRITICAL: Use DEDICATED search window ===
                self.preview_search_results(text)

                # Continue with TTS...
                output_path = "out/search_results.wav"

                if self.synthesize_to_wav(clean_tts_text, output_path, role="text"):
                    with self._play_lock:
                        self._play_token += 1
                        my_token = self._play_token
                        self.interrupt_flag = False
                        self.speaking_flag = True

                    self.set_light("speaking")

                    play_path = output_path
                    if bool(self.echo_enabled_var.get()):
                        try:
                            play_path, _ = self.echo_engine.process_file(output_path, "out/search_results_echo.wav")
                            self.logln("[echo] processed search results -> out/search_results_echo.wav")
                        except Exception as e:
                            self.logln(f"[echo] processing failed: {e} (playing dry)")

                    self.play_wav_with_interrupt(play_path, token=my_token)

            except Exception as e:
                self.logln(f"[search][TTS] Error: {e}")
            finally:
                self.speaking_flag = False
                self.interrupt_flag = False
                self.set_light("idle")

        tts_thread = threading.Thread(target=_tts_worker, daemon=True)
        tts_thread.start()

    # End syththesise_search

    def play_search_results(self, path: str, token=None):
        """Play search results audio with proper interrupt support"""
        try:
            # Use the existing playback infrastructure with token support
            with self._play_lock:
                self._play_token += 1
                my_token = self._play_token
                self.interrupt_flag = False
                self.speaking_flag = True

            self.set_light("speaking")
            self.temporary_mute_for_speech("text")  # Search uses text AI voice
            self.play_wav_with_interrupt(path, token=my_token)

        except Exception as e:
            self.logln(f"[search][playback] Error: {e}")
        finally:
            self.speaking_flag = False
            self.interrupt_flag = False
            self.set_light("idle")

    def normalize_query(self, q: str) -> str:
        """Add date context ONLY for specific time-related queries"""
        ql = q.lower()
        now = datetime.now()

        # Only add dates for explicit time references
        if "today" in ql:
            q += " " + now.strftime("%Y-%m-%d")
        elif "yesterday" in ql:
            q += " " + (now - timedelta(days=1)).strftime("%Y-%m-%d")
        elif "this week" in ql:
            q += " " + now.strftime("week %G-W%V")
        # DON'T add dates for "latest", "recent", "current" etc.

        return q
